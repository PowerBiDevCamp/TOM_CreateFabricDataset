{"cells":[{"cell_type":"code","execution_count":null,"id":"b8105860-3a06-4b08-bcf1-cfcf4f18ed63","metadata":{},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-06T20:56:06.2359853Z","execution_start_time":"2023-10-06T20:55:56.1374159Z","livy_statement_state":"available","parent_msg_id":"93eea794-4ead-4049-a7b5-de61667e1847","queued_time":"2023-10-06T20:55:55.7662476Z","session_id":"2b68fd37-2eac-401a-916d-72047a7db4e0","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":6},"text/plain":["StatementMeta(, 2b68fd37-2eac-401a-916d-72047a7db4e0, 6, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Customers.csv copied to Lakehouse file in OneLake\n","InvoiceDetails.csv copied to Lakehouse file in OneLake\n","Products.csv copied to Lakehouse file in OneLake\n","Invoices.csv copied to Lakehouse file in OneLake\n"]}],"source":["import requests\n","\n","csv_base_url = \"https://github.com/PowerBiDevCamp/Python-In-Fabric-Notebooks/raw/main/ProductSalesData/\"\n","\n","csv_files = { \"Customers.csv\", \"Products.csv\", \"Invoices.csv\", \"InvoiceDetails.csv\" }\n","\n","folder_path = \"Files/landing_zone_sales/\"\n","\n","for csv_file in csv_files:\n","    csv_file_path = csv_base_url + csv_file\n","    with requests.get(csv_file_path) as response:\n","        csv_content = response.content.decode('utf-8-sig')\n","        mssparkutils.fs.put(folder_path + csv_file, csv_content, True)\n","        print(csv_file + \" copied to Lakehouse file in OneLake\")"]},{"cell_type":"code","execution_count":null,"id":"673d17b9-74d6-4c2e-90db-9e377a1853e2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-06T20:56:16.5142759Z","execution_start_time":"2023-10-06T20:56:15.6445849Z","livy_statement_state":"available","parent_msg_id":"806c7cef-7047-4ee2-a251-30265af78bf7","queued_time":"2023-10-06T20:56:15.3429583Z","session_id":"2b68fd37-2eac-401a-916d-72047a7db4e0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-06T20:56:15.993GMT","dataRead":209,"dataWritten":0,"description":"Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/landing_zone_sales/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()","displayName":"showString at NativeMethodAccessorImpl.java:0","jobGroup":"7","jobId":33,"killedTasksSummary":{},"name":"showString at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":10,"stageIds":[56],"status":"SUCCEEDED","submissionTime":"2023-10-06T20:56:15.755GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":7},"text/plain":["StatementMeta(, 2b68fd37-2eac-401a-916d-72047a7db4e0, 7, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["root\n"," |-- ProductId: long (nullable = true)\n"," |-- Product: string (nullable = true)\n"," |-- Category: string (nullable = true)\n","\n","+---------+---------+----------+\n","|ProductId|  Product|  Category|\n","+---------+---------+----------+\n","|        1|   Apples|    Fruits|\n","|        2|  Bananas|    Fruits|\n","|        3|  Oranges|    Fruits|\n","|        4|  Carrots|Vegetables|\n","|        5|Cucumbers|Vegetables|\n","|        6| Potatoes|Vegetables|\n","|        7| Tomatoes|Vegetables|\n","|        8|     Milk|     Dairy|\n","|        9|   Butter|     Dairy|\n","|       10|   Cheese|     Dairy|\n","+---------+---------+----------+\n","\n"]}],"source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_products = StructType([\n","    StructField(\"ProductId\", LongType() ),\n","    StructField(\"Product\", StringType() ),\n","    StructField(\"Category\", StringType() )\n","])\n","\n","df_products = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_products)\n","         .load(\"Files/landing_zone_sales/Products.csv\")\n",")\n","\n","df_products.printSchema()\n","df_products.show()"]},{"cell_type":"code","execution_count":null,"id":"73a7b914-2941-4c49-83ec-56907cf1039e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-06T20:56:21.7449376Z","execution_start_time":"2023-10-06T20:56:20.872498Z","livy_statement_state":"available","parent_msg_id":"387258d2-c689-4947-a9e9-a04d1c94111a","queued_time":"2023-10-06T20:56:20.5608048Z","session_id":"2b68fd37-2eac-401a-916d-72047a7db4e0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-06T20:56:21.121GMT","dataRead":65536,"dataWritten":0,"description":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/landing_zone_sales/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()","displayName":"showString at NativeMethodAccessorImpl.java:0","jobGroup":"8","jobId":34,"killedTasksSummary":{},"name":"showString at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":21,"stageIds":[57],"status":"SUCCEEDED","submissionTime":"2023-10-06T20:56:20.935GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":8},"text/plain":["StatementMeta(, 2b68fd37-2eac-401a-916d-72047a7db4e0, 8, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["root\n"," |-- CustomerId: long (nullable = true)\n"," |-- FirstName: string (nullable = true)\n"," |-- LastName: string (nullable = true)\n"," |-- Country: string (nullable = true)\n"," |-- City: string (nullable = true)\n"," |-- DOB: date (nullable = true)\n","\n","+----------+----------+---------+--------+----------+----------+\n","|CustomerId| FirstName| LastName| Country|      City|       DOB|\n","+----------+----------+---------+--------+----------+----------+\n","|         1|   Yolanda|   Wagner| Belgium|  Brussels|1970-02-24|\n","|         2|    Arnold|   Harmon| England|    London|1943-11-01|\n","|         3|      Minh|    Casey| Ireland|  Limerick|1976-11-24|\n","|         4|   Russell| McIntyre|Portugal|     Braga|1993-08-05|\n","|         5|  Angelina| Santiago| England|    London|1978-08-22|\n","|         6|   Sabrina|   Conway| England|Birmingham|1963-06-25|\n","|         7|Jacqueline|Zimmerman| Belgium|  Brussels|2003-05-04|\n","|         8|     Dewey|  Francis| England| Liverpool|1957-01-10|\n","|         9|     Haley| McDowell| Belgium|  Brussels|1987-04-25|\n","|        10|  Brittany|     Tate| England| Liverpool|1991-04-07|\n","|        11|     Dario|    Morse|Portugal|     Porto|1967-06-02|\n","|        12|      Josh|   Conrad| England| Liverpool|1983-10-18|\n","|        13|   Coleman|     Rios| England|Birmingham|1951-01-17|\n","|        14|     Shari|   Nieves| England| Liverpool|1983-05-04|\n","|        15|      Keri|    Reyes| England|    London|1996-02-09|\n","|        16|      Lacy|    Cross| Belgium|   Antwerp|1998-04-03|\n","|        17|    Willie|   Bonner| England| Liverpool|1950-08-02|\n","|        18|    Silvia|  Justice| England| Liverpool|1966-02-03|\n","|        19|      Jody|   Vinson| Belgium|  Brussels|1961-04-06|\n","|        20|  Clifford|     Kent| England| Liverpool|1991-11-14|\n","+----------+----------+---------+--------+----------+----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined with StructType and StructField \n","schema_customers = StructType([\n","    StructField(\"CustomerId\", LongType() ),\n","    StructField(\"FirstName\", StringType() ),\n","    StructField(\"LastName\", StringType() ),\n","    StructField(\"Country\", StringType() ),\n","    StructField(\"City\", StringType() ),\n","    StructField(\"DOB\", DateType() ),\n","])\n","\n","df_customers = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_customers)\n","         .option(\"dateFormat\", \"M/d/yyyy\")\n","         .option(\"inferSchema\", \"true\")\n","         .load(\"Files/landing_zone_sales/Customers.csv\")\n",")\n","\n","df_customers.printSchema()\n","df_customers.show()"]},{"cell_type":"code","execution_count":null,"id":"37d1b67b-1893-446f-926a-7f84b5774a81","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-06T21:01:33.1048759Z","execution_start_time":"2023-10-06T21:01:32.2347366Z","livy_statement_state":"available","parent_msg_id":"d2c0fbbe-3be6-4d48-a92d-07e912098fda","queued_time":"2023-10-06T21:01:31.9172332Z","session_id":"2b68fd37-2eac-401a-916d-72047a7db4e0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-06T21:01:32.430GMT","dataRead":65536,"dataWritten":0,"description":"Job group for statement 11:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/landing_zone_sales/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()","displayName":"showString at NativeMethodAccessorImpl.java:0","jobGroup":"11","jobId":39,"killedTasksSummary":{},"name":"showString at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":21,"stageIds":[62],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:01:32.317GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":11},"text/plain":["StatementMeta(, 2b68fd37-2eac-401a-916d-72047a7db4e0, 11, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["root\n"," |-- InvoiceId: long (nullable = true)\n"," |-- Date: date (nullable = true)\n"," |-- TotalSalesAmount: float (nullable = true)\n"," |-- CustomerId: long (nullable = true)\n","\n","+---------+----------+----------------+----------+\n","|InvoiceId|      Date|TotalSalesAmount|CustomerId|\n","+---------+----------+----------------+----------+\n","|        1|2020-01-01|            72.0|         1|\n","|        2|2020-01-01|            35.0|         2|\n","|        3|2020-01-01|             5.0|         3|\n","|        4|2020-01-01|           58.75|         4|\n","|        5|2020-01-01|            39.1|         5|\n","|        6|2020-01-01|            37.5|         6|\n","|        7|2020-01-01|           22.15|         7|\n","|        8|2020-01-01|            58.1|         8|\n","|        9|2020-01-01|           51.75|         9|\n","|       10|2020-01-01|          120.75|        10|\n","|       11|2020-01-02|           78.95|        11|\n","|       12|2020-01-02|            54.1|        12|\n","|       13|2020-01-02|             7.5|        13|\n","|       14|2020-01-02|            49.2|        14|\n","|       15|2020-01-02|            2.85|        15|\n","|       16|2020-01-02|             2.5|        16|\n","|       17|2020-01-02|            24.0|        17|\n","|       18|2020-01-02|            34.4|        18|\n","|       19|2020-01-02|            79.6|        19|\n","|       20|2020-01-02|           31.25|        20|\n","+---------+----------+----------------+----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_invoices = StructType([\n","    StructField(\"InvoiceId\", LongType() ),\n","    StructField(\"Date\", DateType() ),\n","    StructField(\"TotalSalesAmount\", FloatType() ),\n","    StructField(\"CustomerId\", LongType() )\n","])\n","\n","df_invoices = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_invoices)\n","         .option(\"dateFormat\", \"MM/dd/yyyy\")\n","         .option(\"inferSchema\", \"true\") \n","         .load(\"Files/landing_zone_sales/Invoices.csv\")\n",")\n","\n","df_invoices.printSchema()\n","df_invoices.show()"]},{"cell_type":"code","execution_count":null,"id":"4cf1db35-9943-4e70-ba42-fa48cd656c95","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-06T21:01:57.1270499Z","execution_start_time":"2023-10-06T21:01:56.2588437Z","livy_statement_state":"available","parent_msg_id":"61848407-9a72-45b8-a501-530719408ae5","queued_time":"2023-10-06T21:01:55.9121964Z","session_id":"2b68fd37-2eac-401a-916d-72047a7db4e0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-06T21:01:56.440GMT","dataRead":65536,"dataWritten":0,"description":"Job group for statement 12:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/landing_zone_sales/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()","displayName":"showString at NativeMethodAccessorImpl.java:0","jobGroup":"12","jobId":40,"killedTasksSummary":{},"name":"showString at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":21,"stageIds":[63],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:01:56.307GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":12},"text/plain":["StatementMeta(, 2b68fd37-2eac-401a-916d-72047a7db4e0, 12, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["root\n"," |-- Id: long (nullable = true)\n"," |-- Quantity: long (nullable = true)\n"," |-- SalesAmount: float (nullable = true)\n"," |-- InvoiceId: long (nullable = true)\n"," |-- ProductId: long (nullable = true)\n","\n","+---+--------+-----------+---------+---------+\n","| Id|Quantity|SalesAmount|InvoiceId|ProductId|\n","+---+--------+-----------+---------+---------+\n","|  2|       2|        4.5|        1|        5|\n","|  2|      30|       67.5|        1|        9|\n","|  3|      20|       35.0|        2|        7|\n","|  4|       4|        5.0|        3|        3|\n","|  5|      20|       25.0|        4|        6|\n","|  5|       6|       10.5|        4|        7|\n","|  5|      31|      23.25|        4|        1|\n","|  6|       3|       2.85|        5|        2|\n","|  6|      29|      36.25|        5|        3|\n","|  7|      15|       37.5|        6|        8|\n","|  8|      17|      16.15|        7|        4|\n","|  8|       8|        6.0|        7|        1|\n","|  9|      28|       26.6|        8|        2|\n","|  9|      18|       31.5|        8|        7|\n","| 10|      25|      23.75|        9|        4|\n","| 10|      16|       28.0|        9|        7|\n","| 11|       7|      12.25|       10|        7|\n","| 11|      33|      57.75|       10|        7|\n","| 11|      29|      50.75|       10|        7|\n","| 12|      31|      54.25|       11|        7|\n","+---+--------+-----------+---------+---------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_invoice_details = StructType([\n","    StructField(\"Id\", LongType() ),\n","    StructField(\"Quantity\", LongType() ),\n","    StructField(\"SalesAmount\", FloatType() ),\n","    StructField(\"InvoiceId\", LongType() ),\n","    StructField(\"ProductId\", LongType() )\n","])\n","\n","df_invoice_details = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_invoice_details)\n","         .option(\"dateFormat\", \"MM/dd/yyyy\")\n","         .option(\"inferSchema\", \"true\") \n","         .load(\"Files/landing_zone_sales/InvoiceDetails.csv\")\n",")\n","\n","df_invoice_details.printSchema()\n","df_invoice_details.show()"]},{"cell_type":"code","execution_count":29,"id":"46e2de00-a6f7-4c97-857d-8de8940e5fa6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-07T18:13:22.2334817Z","execution_start_time":"2023-10-07T18:13:21.9254618Z","livy_statement_state":"available","parent_msg_id":"b6d8b95d-213b-4003-8c78-8e535d2076ed","queued_time":"2023-10-07T18:13:21.5992112Z","session_id":"86c586d0-fd21-4305-99ae-bcbfd268cf34","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":31},"text/plain":["StatementMeta(, 86c586d0-fd21-4305-99ae-bcbfd268cf34, 31, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"ename":"NameError","evalue":"name 'df_products' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_products\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwriteSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTables/bronze_products\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_customers\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwriteSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTables/bronze_customers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_invoices\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwriteSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTables/bronze_invoices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'df_products' is not defined"]}],"source":["df_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\n","df_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\n","df_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\n","df_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")"]},{"cell_type":"code","execution_count":null,"id":"deae8c98-52fd-4f2a-aad9-2cef0bc118f9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-06T21:02:38.7405245Z","execution_start_time":"2023-10-06T21:02:33.8153094Z","livy_statement_state":"available","parent_msg_id":"4e2fdc3b-4930-4930-b0df-0d825911daef","queued_time":"2023-10-06T21:02:33.4738227Z","session_id":"2b68fd37-2eac-401a-916d-72047a7db4e0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-06T21:02:37.777GMT","dataRead":1444,"dataWritten":0,"description":"Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","displayName":"showString at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":82,"killedTasksSummary":{},"name":"showString at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":10,"stageIds":[141],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:37.694GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:37.648GMT","dataRead":2736,"dataWritten":0,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"14","jobId":81,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":8,"stageIds":[139,140],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:37.423GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:37.221GMT","dataRead":4349,"dataWritten":0,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"14","jobId":80,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":56,"numTasks":57,"rowCount":50,"stageIds":[136,137,138],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:37.182GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:37.158GMT","dataRead":9706,"dataWritten":4349,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"14","jobId":79,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":70,"stageIds":[135,134],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:36.639GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:36.513GMT","dataRead":7511,"dataWritten":9706,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"14","jobId":78,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":40,"stageIds":[133],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:36.396GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:35.990GMT","dataRead":2429,"dataWritten":0,"description":"Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"14","jobId":77,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":5,"numTasks":55,"rowCount":7,"stageIds":[132,131],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:35.829GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:35.727GMT","dataRead":338,"dataWritten":1874,"description":"Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":76,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":20,"stageIds":[129,130],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:35.522GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:35.486GMT","dataRead":1444,"dataWritten":338,"description":"Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":75,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":20,"stageIds":[128],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:35.401GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:35.346GMT","dataRead":2736,"dataWritten":0,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"14","jobId":74,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":8,"stageIds":[126,127],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:35.190GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:34.957GMT","dataRead":4349,"dataWritten":0,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 4","displayName":"toString at String.java:2994","jobGroup":"14","jobId":73,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":55,"numTasks":56,"rowCount":50,"stageIds":[125,123,124],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:34.927GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:34.902GMT","dataRead":8137,"dataWritten":4349,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 4","displayName":"toString at String.java:2994","jobGroup":"14","jobId":72,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":5,"numTasks":55,"rowCount":67,"stageIds":[121,122],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:34.480GMT","usageDescription":""},{"completionTime":"2023-10-06T21:02:34.326GMT","dataRead":6417,"dataWritten":8137,"description":"Delta: Job group for statement 14:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 4","displayName":"toString at String.java:2994","jobGroup":"14","jobId":71,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":5,"numCompletedStages":1,"numCompletedTasks":5,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":5,"rowCount":34,"stageIds":[120],"status":"SUCCEEDED","submissionTime":"2023-10-06T21:02:34.195GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":12,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":14},"text/plain":["StatementMeta(, 2b68fd37-2eac-401a-916d-72047a7db4e0, 14, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["+---------+---------+----------+\n","|ProductId|  Product|  Category|\n","+---------+---------+----------+\n","|        1|   Apples|    Fruits|\n","|        2|  Bananas|    Fruits|\n","|        3|  Oranges|    Fruits|\n","|        4|  Carrots|Vegetables|\n","|        5|Cucumbers|Vegetables|\n","|        6| Potatoes|Vegetables|\n","|        7| Tomatoes|Vegetables|\n","|        8|     Milk|     Dairy|\n","|        9|   Butter|     Dairy|\n","|       10|   Cheese|     Dairy|\n","+---------+---------+----------+\n","\n"]}],"source":["df_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n","\n","df_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n","\n","df_silver_products.show()"]},{"cell_type":"code","execution_count":30,"id":"87a11595-0a6c-4b16-aa8d-8883217810e0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-07T18:19:38.3868677Z","execution_start_time":"2023-10-07T18:19:31.4353341Z","livy_statement_state":"available","parent_msg_id":"cc34c13f-3942-4fca-8680-08eb7c832a33","queued_time":"2023-10-07T18:19:31.083125Z","session_id":"86c586d0-fd21-4305-99ae-bcbfd268cf34","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-07T18:19:36.476GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","displayName":"showString at <unknown>:0","jobGroup":"32","jobId":355,"killedTasksSummary":{},"name":"showString at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[574],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:36.305GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:36.236GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"32","jobId":354,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":0,"stageIds":[572,573],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:36.094GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:35.892GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 6","displayName":"toString at String.java:2994","jobGroup":"32","jobId":353,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":57,"numTasks":58,"rowCount":0,"stageIds":[571,569,570],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:35.870GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:35.855GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 6","displayName":"toString at String.java:2994","jobGroup":"32","jobId":352,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":7,"numTasks":57,"rowCount":0,"stageIds":[568,567],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:35.467GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:35.309GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 6","displayName":"toString at String.java:2994","jobGroup":"32","jobId":351,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":7,"numCompletedStages":1,"numCompletedTasks":7,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":7,"rowCount":0,"stageIds":[566],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:35.219GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:34.849GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"32","jobId":350,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":0,"stageIds":[564,565],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:34.751GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:34.651GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"32","jobId":349,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":0,"stageIds":[562,563],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:33.943GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:33.904GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"32","jobId":348,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[561],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:33.553GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:33.499GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"32","jobId":347,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":0,"stageIds":[560,559],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:33.384GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:33.224GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"32","jobId":346,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":56,"numTasks":57,"rowCount":0,"stageIds":[556,557,558],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:33.202GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:33.189GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"32","jobId":345,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":0,"stageIds":[554,555],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:32.849GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:32.744GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"32","jobId":344,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":0,"stageIds":[553],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:32.659GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:32.318GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"32","jobId":343,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":56,"numTasks":57,"rowCount":0,"stageIds":[550,551,552],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:32.289GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:32.273GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"32","jobId":342,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":0,"stageIds":[548,549],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:31.864GMT","usageDescription":""},{"completionTime":"2023-10-07T18:19:31.762GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 32:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"32","jobId":341,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":0,"stageIds":[547],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:19:31.642GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":15,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":32},"text/plain":["StatementMeta(, 86c586d0-fd21-4305-99ae-bcbfd268cf34, 32, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["root\n"," |-- CustomerId: long (nullable = true)\n"," |-- Country: string (nullable = true)\n"," |-- City: string (nullable = true)\n"," |-- DOB: date (nullable = true)\n"," |-- Customer: string (nullable = false)\n"," |-- Age: long (nullable = true)\n","\n","+----------+-------+-------+----------+-----------------+---+\n","|CustomerId|Country|   City|       DOB|         Customer|Age|\n","+----------+-------+-------+----------+-----------------+---+\n","|        16|Belgium|Antwerp|1998-04-03|       Lacy Cross| 25|\n","|        57|Belgium|Antwerp|1992-04-02|     Cheryl Morse| 31|\n","|        77|Belgium|Antwerp|1970-09-17|      Dolly Boyer| 53|\n","|       133|Belgium|Antwerp|1991-06-13| Alyssa Frederick| 32|\n","|       137|Belgium|Antwerp|1992-08-17|   Maxine Walters| 31|\n","|       141|Belgium|Antwerp|1951-06-06|    Burl McCarthy| 72|\n","|       147|Belgium|Antwerp|1983-11-20|     Eloise Bruce| 39|\n","|       184|Belgium|Antwerp|2004-10-23|   Gwen Whitehead| 18|\n","|       211|Belgium|Antwerp|1952-03-01|    Prince Franco| 71|\n","|       221|Belgium|Antwerp|1944-02-21|  Guadalupe Lewis| 79|\n","|       232|Belgium|Antwerp|1956-08-13|      Willie Pate| 67|\n","|       268|Belgium|Antwerp|1963-06-17|     Graham Klein| 60|\n","|       275|Belgium|Antwerp|1973-06-14|Houston Alexander| 50|\n","|       300|Belgium|Antwerp|1945-07-07|    Lindsay Pitts| 78|\n","|       319|Belgium|Antwerp|2002-02-22|         Elsa Cox| 21|\n","|       329|Belgium|Antwerp|1989-03-20|   Rosanna Hodges| 34|\n","|       359|Belgium|Antwerp|1999-08-09|    Hunter Newton| 24|\n","|       413|Belgium|Antwerp|1966-01-06|     Alana Graves| 57|\n","|       426|Belgium|Antwerp|1960-07-19|     Jamie Gibson| 63|\n","|       427|Belgium|Antwerp|1957-11-19| Garrett Mitchell| 65|\n","+----------+-------+-------+----------+-----------------+---+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n","\n","df_silver_customers = (\n","\n","    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n","            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n","            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n","            .drop('FirstName', 'LastName')\n",")\n","\n","df_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n","\n","df_silver_customers.printSchema()\n","df_silver_customers.show()"]},{"cell_type":"code","execution_count":27,"id":"f98476ca-520e-441b-94e9-734b9f4e8e29","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-07T18:03:08.9209915Z","execution_start_time":"2023-10-07T18:03:00.52693Z","livy_statement_state":"available","parent_msg_id":"80a4fea3-8f07-4901-8fd7-26cad792b517","queued_time":"2023-10-07T18:03:00.1576324Z","session_id":"86c586d0-fd21-4305-99ae-bcbfd268cf34","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-07T18:03:07.177GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","displayName":"showString at <unknown>:0","jobGroup":"29","jobId":323,"killedTasksSummary":{},"name":"showString at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":0,"stageIds":[518],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:07.027GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:07.023GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","displayName":"showString at <unknown>:0","jobGroup":"29","jobId":322,"killedTasksSummary":{},"name":"showString at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[517],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:06.879GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:06.315GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"29","jobId":320,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":0,"stageIds":[514,515],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:06.163GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:06.088GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"29","jobId":319,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":0,"stageIds":[513,512],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:05.985GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:05.866GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 11","displayName":"toString at String.java:2994","jobGroup":"29","jobId":318,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":0,"stageIds":[509,510,511],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:05.811GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:05.793GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 11","displayName":"toString at String.java:2994","jobGroup":"29","jobId":317,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":0,"stageIds":[507,508],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:05.362GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:05.235GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 11","displayName":"toString at String.java:2994","jobGroup":"29","jobId":316,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":0,"stageIds":[506],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:05.120GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:04.676GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"29","jobId":315,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":0,"stageIds":[504,505],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:04.572GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:04.481GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"29","jobId":314,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":4,"rowCount":0,"stageIds":[502,503],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:03.350GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:03.314GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"29","jobId":313,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":0,"stageIds":[501],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:02.564GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:01.996GMT","dataRead":1307,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"29","jobId":311,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":1,"stageIds":[499,498],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:01.899GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:01.794GMT","dataRead":1322,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"29","jobId":310,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":1,"stageIds":[496,497],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:01.684GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:01.583GMT","dataRead":4424,"dataWritten":0,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 10","displayName":"toString at String.java:2994","jobGroup":"29","jobId":309,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[494,495,493],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:01.556GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:01.534GMT","dataRead":4289,"dataWritten":4424,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 10","displayName":"toString at String.java:2994","jobGroup":"29","jobId":308,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":63,"stageIds":[491,492],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:01.146GMT","usageDescription":""},{"completionTime":"2023-10-07T18:03:01.030GMT","dataRead":26597,"dataWritten":4289,"description":"Delta: Job group for statement 29:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 10","displayName":"toString at String.java:2994","jobGroup":"29","jobId":307,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":26,"stageIds":[490],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:03:00.858GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":15,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":29},"text/plain":["StatementMeta(, 86c586d0-fd21-4305-99ae-bcbfd268cf34, 29, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["root\n"," |-- Date: date (nullable = true)\n"," |-- DateKey: integer (nullable = true)\n"," |-- CustomerId: long (nullable = true)\n"," |-- ProductId: long (nullable = true)\n"," |-- Sales: float (nullable = true)\n"," |-- Quantity: long (nullable = true)\n","\n","+----------+--------+----------+---------+-----+--------+\n","|      Date| DateKey|CustomerId|ProductId|Sales|Quantity|\n","+----------+--------+----------+---------+-----+--------+\n","|2020-02-24|20200224|       606|       10| 90.0|      24|\n","|2020-03-06|20200306|       776|       10| 90.0|      24|\n","|2020-03-06|20200306|       789|       10| 90.0|      24|\n","|2020-03-07|20200307|       803|       10| 90.0|      24|\n","|2020-03-18|20200318|      1079|       10| 90.0|      24|\n","|2020-04-02|20200402|      1663|       10| 90.0|      24|\n","|2020-04-11|20200411|      1992|       10| 90.0|      24|\n","|2020-04-12|20200412|      2079|       10| 90.0|      24|\n","|2020-04-14|20200414|      2244|       10| 90.0|      24|\n","|2020-04-29|20200429|      3055|       10| 90.0|      24|\n","|2020-05-03|20200503|      3389|       10| 90.0|      24|\n","|2020-05-07|20200507|      3633|       10| 90.0|      24|\n","|2020-05-09|20200509|      3773|       10| 90.0|      24|\n","|2020-05-11|20200511|      3823|       10| 90.0|      24|\n","|2020-05-11|20200511|      3828|       10| 90.0|      24|\n","|2020-05-13|20200513|      4036|       10| 90.0|      24|\n","|2020-05-19|20200519|      4407|       10| 90.0|      24|\n","|2020-05-21|20200521|      4551|       10| 90.0|      24|\n","|2020-05-23|20200523|      4638|       10| 90.0|      24|\n","|2020-05-26|20200526|      4774|       10| 90.0|      24|\n","+----------+--------+----------+---------+-----+--------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import col, desc, concat, lit, floor, datediff\n","from pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n","\n","df_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\n","df_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n","\n","df_silver_sales = (\n","    df_bronze_invoice_details\n","            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n","            .withColumnRenamed('SalesAmount', 'Sales')\n","            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n","                               (month(col('Date'))*100) + \n","                               (dayofmonth(col('Date')))   )\n","\n","            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n","            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n",")\n","\n","df_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n","\n","df_silver_sales.printSchema()\n","df_silver_sales.show()"]},{"cell_type":"code","execution_count":28,"id":"58438ae5-1d64-489c-9d0c-44edece5b09b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-10-07T18:05:13.9672933Z","execution_start_time":"2023-10-07T18:05:08.9378562Z","livy_statement_state":"available","parent_msg_id":"25cd26a6-2621-4e91-8e47-24a389420bd0","queued_time":"2023-10-07T18:05:08.5893518Z","session_id":"86c586d0-fd21-4305-99ae-bcbfd268cf34","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-10-07T18:05:13.487GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"showString at <unknown>:0","jobGroup":"30","jobId":340,"killedTasksSummary":{},"name":"showString at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[546],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:13.477GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:13.434GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"30","jobId":339,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":0,"stageIds":[545,543,544],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:13.410GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:13.393GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"30","jobId":338,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":0,"stageIds":[542,541],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:12.990GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:12.821GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"30","jobId":337,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":0,"stageIds":[540],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:12.773GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:12.400GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"30","jobId":336,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":0,"stageIds":[538,539],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:12.315GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:12.207GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"30","jobId":335,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":0,"stageIds":[536,537],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:11.979GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:11.929GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"30","jobId":334,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":0,"stageIds":[535],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:11.875GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:11.710GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:9","jobGroup":"30","jobId":333,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:9","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":4,"rowCount":0,"stageIds":[533,534],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:11.695GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:11.686GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:9","jobGroup":"30","jobId":332,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:9","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":0,"stageIds":[532],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:11.351GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:10.840GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:9","jobGroup":"30","jobId":330,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:9","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":0,"stageIds":[529,530],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:10.697GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:10.624GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:9","jobGroup":"30","jobId":329,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:9","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":0,"stageIds":[527,528],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:10.472GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:10.373GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:8","jobGroup":"30","jobId":328,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:8","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":4,"rowCount":0,"stageIds":[525,526],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:10.363GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:10.353GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:8","jobGroup":"30","jobId":327,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:8","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":0,"stageIds":[524],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:10.065GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:09.506GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:8","jobGroup":"30","jobId":325,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:8","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":0,"stageIds":[521,522],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:09.406GMT","usageDescription":""},{"completionTime":"2023-10-07T18:05:09.334GMT","dataRead":0,"dataWritten":0,"description":"Delta: Job group for statement 30:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","displayName":"collect at /tmp/ipykernel_8225/2758995502.py:8","jobGroup":"30","jobId":324,"killedTasksSummary":{},"name":"collect at /tmp/ipykernel_8225/2758995502.py:8","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":0,"stageIds":[520,519],"status":"SUCCEEDED","submissionTime":"2023-10-07T18:05:09.217GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":15,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":30},"text/plain":["StatementMeta(, 86c586d0-fd21-4305-99ae-bcbfd268cf34, 30, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n","|      Date| DateKey|Year|Quarter|  Month|Day|MonthInYear|MonthInYearSort|DayOfWeek|DayOfWeekSort|\n","+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n","|2020-01-01|20200101|2020|2020-01|2020-01|  1|    January|              1|Wednesday|            4|\n","|2020-01-02|20200102|2020|2020-01|2020-01|  2|    January|              1| Thursday|            5|\n","|2020-01-03|20200103|2020|2020-01|2020-01|  3|    January|              1|   Friday|            6|\n","|2020-01-04|20200104|2020|2020-01|2020-01|  4|    January|              1| Saturday|            7|\n","|2020-01-05|20200105|2020|2020-01|2020-01|  5|    January|              1|   Sunday|            1|\n","|2020-01-06|20200106|2020|2020-01|2020-01|  6|    January|              1|   Monday|            2|\n","|2020-01-07|20200107|2020|2020-01|2020-01|  7|    January|              1|  Tuesday|            3|\n","|2020-01-08|20200108|2020|2020-01|2020-01|  8|    January|              1|Wednesday|            4|\n","|2020-01-09|20200109|2020|2020-01|2020-01|  9|    January|              1| Thursday|            5|\n","|2020-01-10|20200110|2020|2020-01|2020-01| 10|    January|              1|   Friday|            6|\n","|2020-01-11|20200111|2020|2020-01|2020-01| 11|    January|              1| Saturday|            7|\n","|2020-01-12|20200112|2020|2020-01|2020-01| 12|    January|              1|   Sunday|            1|\n","|2020-01-13|20200113|2020|2020-01|2020-01| 13|    January|              1|   Monday|            2|\n","|2020-01-14|20200114|2020|2020-01|2020-01| 14|    January|              1|  Tuesday|            3|\n","|2020-01-15|20200115|2020|2020-01|2020-01| 15|    January|              1|Wednesday|            4|\n","|2020-01-16|20200116|2020|2020-01|2020-01| 16|    January|              1| Thursday|            5|\n","|2020-01-17|20200117|2020|2020-01|2020-01| 17|    January|              1|   Friday|            6|\n","|2020-01-18|20200118|2020|2020-01|2020-01| 18|    January|              1| Saturday|            7|\n","|2020-01-19|20200119|2020|2020-01|2020-01| 19|    January|              1|   Sunday|            1|\n","|2020-01-20|20200120|2020|2020-01|2020-01| 20|    January|              1|   Monday|            2|\n","+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["import pyspark.pandas as ps\n","from datetime import datetime, timedelta, date\n","import os\n","import pandas as pd\n","\n","from pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n","\n","first_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\n","last_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n","\n","start_date = date(first_sales_date.year, 1, 1)\n","end_date = date(last_sales_date.year, 12, 31)\n","\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","df_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n","\n","df_calendar_spark = (\n","     spark.createDataFrame(df_calendar_ps)\n","       .withColumnRenamed(\"0\", \"timestamp\")\n","       .withColumn(\"Date\", to_date(col('timestamp')))\n","       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n","                              (month(col('timestamp'))*100) + \n","                              (dayofmonth(col('timestamp')))   )\n","       .withColumn(\"Year\", year(col('timestamp'))  )\n","       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n","       .withColumn(\"Month\", date_format(col('timestamp'),'yyyy-MM')  )\n","       .withColumn(\"Day\", dayofmonth(col('timestamp'))  )\n","       .withColumn(\"MonthInYear\", date_format(col('timestamp'),'MMMM')  )\n","       .withColumn(\"MonthInYearSort\", month(col('timestamp'))  )\n","       .withColumn(\"DayOfWeek\", date_format(col('timestamp'),'EEEE')  )\n","       .withColumn(\"DayOfWeekSort\", dayofweek(col('timestamp')))\n","       .drop('timestamp')\n",")\n","\n","df_calendar_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/calendar\")\n","df_calendar_spark.show()"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"9b8dffc4-bcf0-4908-a244-cec42dcb2503","default_lakehouse_name":"TommyBoyLakehouse","default_lakehouse_workspace_id":"d9c4b9ba-9c7a-411b-bf63-79d13f0d6947","known_lakehouses":[{"id":"9b8dffc4-bcf0-4908-a244-cec42dcb2503"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
