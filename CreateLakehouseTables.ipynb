{"cells":[{"cell_type":"code","source":["import requests\n","\n","csv_base_url = \"https://github.com/PowerBiDevCamp/TOM_CreateFabricDataset/raw/main/ProductSalesData/\"\n","\n","csv_files = { \"Customers.csv\", \"Products.csv\", \"Invoices.csv\", \"InvoiceDetails.csv\" }\n","\n","folder_path = \"Files/bronze_landing_zone/\"\n","\n","for csv_file in csv_files:\n","    csv_file_path = csv_base_url + csv_file\n","    with requests.get(csv_file_path) as response:\n","        csv_content = response.content.decode('utf-8-sig')\n","        mssparkutils.fs.put(folder_path + csv_file, csv_content, True)\n","        print(csv_file + \" copied to Lakehouse file in OneLake\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T21:42:01.4742747Z","session_start_time":"2023-10-10T21:42:01.7136428Z","execution_start_time":"2023-10-10T21:42:12.0629838Z","execution_finish_time":"2023-10-10T21:42:23.009713Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ba31c7f5-63de-43b9-b1fc-ad4dd296dda5"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 3, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Customers.csv copied to Lakehouse file in OneLake\nInvoiceDetails.csv copied to Lakehouse file in OneLake\nProducts.csv copied to Lakehouse file in OneLake\nInvoices.csv copied to Lakehouse file in OneLake\n"]}],"execution_count":1,"metadata":{},"id":"b8105860-3a06-4b08-bcf1-cfcf4f18ed63"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_products = StructType([\n","    StructField(\"ProductId\", LongType() ),\n","    StructField(\"Product\", StringType() ),\n","    StructField(\"Category\", StringType() )\n","])\n","\n","df_products = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_products)\n","         .load(\"Files/bronze_landing_zone/Products.csv\")\n",")\n","\n","df_products.printSchema()\n","df_products.show()\n","\n","( df_products.write\n","             .mode(\"overwrite\")\n","             .option(\"overwriteSchema\", \"True\")\n","             .format(\"delta\")\n","             .save(\"Tables/silver_products\")\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T21:49:18.6171022Z","session_start_time":null,"execution_start_time":"2023-10-10T21:49:18.9632004Z","execution_finish_time":"2023-10-10T21:49:22.6624209Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4329,"rowCount":50,"usageDescription":"","jobId":21,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:49:21.708GMT","completionTime":"2023-10-10T21:49:21.758GMT","stageIds":[30,31,29],"jobGroup":"5","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4329,"dataRead":1499,"rowCount":54,"usageDescription":"","jobId":20,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:49:20.789GMT","completionTime":"2023-10-10T21:49:21.677GMT","stageIds":[27,28],"jobGroup":"5","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1499,"dataRead":1364,"rowCount":8,"usageDescription":"","jobId":19,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:49:20.528GMT","completionTime":"2023-10-10T21:49:20.618GMT","stageIds":[26],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":18,"name":"","description":"Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n)","submissionTime":"2023-10-10T21:49:19.975GMT","completionTime":"2023-10-10T21:49:19.975GMT","stageIds":[],"jobGroup":"5","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1874,"dataRead":338,"rowCount":20,"usageDescription":"","jobId":17,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n)","submissionTime":"2023-10-10T21:49:19.468GMT","completionTime":"2023-10-10T21:49:19.777GMT","stageIds":[24,25],"jobGroup":"5","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":338,"dataRead":209,"rowCount":20,"usageDescription":"","jobId":16,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n)","submissionTime":"2023-10-10T21:49:19.354GMT","completionTime":"2023-10-10T21:49:19.421GMT","stageIds":[23],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":209,"rowCount":10,"usageDescription":"","jobId":15,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/bronze_landing_zone/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()\n\n\n( df_products.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_products\")\n)","submissionTime":"2023-10-10T21:49:19.048GMT","completionTime":"2023-10-10T21:49:19.159GMT","stageIds":[22],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c94e99a3-795a-4d8c-9251-6c0ae57a902f"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- ProductId: long (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n\n+---------+---------+----------+\n|ProductId|  Product|  Category|\n+---------+---------+----------+\n|        1|   Apples|    Fruits|\n|        2|  Bananas|    Fruits|\n|        3|  Oranges|    Fruits|\n|        4|  Carrots|Vegetables|\n|        5|Cucumbers|Vegetables|\n|        6| Potatoes|Vegetables|\n|        7| Tomatoes|Vegetables|\n|        8|     Milk|     Dairy|\n|        9|   Butter|     Dairy|\n|       10|   Cheese|     Dairy|\n+---------+---------+----------+\n\n"]}],"execution_count":3,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"673d17b9-74d6-4c2e-90db-9e377a1853e2"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined with StructType and StructField \n","schema_customers = StructType([\n","    StructField(\"CustomerId\", LongType() ),\n","    StructField(\"FirstName\", StringType() ),\n","    StructField(\"LastName\", StringType() ),\n","    StructField(\"Country\", StringType() ),\n","    StructField(\"City\", StringType() ),\n","    StructField(\"DOB\", DateType() ),\n","])\n","\n","df_customers = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_customers)\n","         .option(\"dateFormat\", \"M/d/yyyy\")\n","         .option(\"inferSchema\", \"true\")\n","         .load(\"Files/bronze_landing_zone/Customers.csv\")\n",")\n","\n","df_customers.printSchema()\n","df_customers.show()\n","\n","( df_customers.write\n","              .mode(\"overwrite\")\n","              .option(\"overwriteSchema\", \"True\")\n","              .format(\"delta\")\n","              .save(\"Tables/silver_customers\")\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T21:50:18.7353909Z","session_start_time":null,"execution_start_time":"2023-10-10T21:50:19.0714859Z","execution_finish_time":"2023-10-10T21:50:24.1341618Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4372,"rowCount":50,"usageDescription":"","jobId":28,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:50:22.625GMT","completionTime":"2023-10-10T21:50:22.675GMT","stageIds":[39,40,41],"jobGroup":"6","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4372,"dataRead":1657,"rowCount":54,"usageDescription":"","jobId":27,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:50:21.810GMT","completionTime":"2023-10-10T21:50:22.603GMT","stageIds":[37,38],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1657,"dataRead":1788,"rowCount":8,"usageDescription":"","jobId":26,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:50:21.470GMT","completionTime":"2023-10-10T21:50:21.534GMT","stageIds":[36],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":25,"name":"","description":"Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n)","submissionTime":"2023-10-10T21:50:21.020GMT","completionTime":"2023-10-10T21:50:21.020GMT","stageIds":[],"jobGroup":"6","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":2019024,"dataRead":8075798,"rowCount":434754,"usageDescription":"","jobId":24,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n)","submissionTime":"2023-10-10T21:50:20.084GMT","completionTime":"2023-10-10T21:50:20.875GMT","stageIds":[34,35],"jobGroup":"6","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":8075798,"dataRead":10387094,"rowCount":434754,"usageDescription":"","jobId":23,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n)","submissionTime":"2023-10-10T21:50:19.559GMT","completionTime":"2023-10-10T21:50:20.034GMT","stageIds":[33],"jobGroup":"6","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":21,"usageDescription":"","jobId":22,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/bronze_landing_zone/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()\n\n( df_customers.write\n              .mode(\"overwrite\")\n              .option(\"overwriteSchema\", \"True\")\n              .format(\"delta\")\n              .save(\"Tables/silver_customers\")\n)","submissionTime":"2023-10-10T21:50:19.173GMT","completionTime":"2023-10-10T21:50:19.354GMT","stageIds":[32],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"cb041cf4-c274-40ba-80da-d373fc1babff"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- CustomerId: long (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- City: string (nullable = true)\n |-- DOB: date (nullable = true)\n\n+----------+----------+---------+--------+----------+----------+\n|CustomerId| FirstName| LastName| Country|      City|       DOB|\n+----------+----------+---------+--------+----------+----------+\n|         1|   Yolanda|   Wagner| Belgium|  Brussels|1970-02-24|\n|         2|    Arnold|   Harmon| England|    London|1943-11-01|\n|         3|      Minh|    Casey| Ireland|  Limerick|1976-11-24|\n|         4|   Russell| McIntyre|Portugal|     Braga|1993-08-05|\n|         5|  Angelina| Santiago| England|    London|1978-08-22|\n|         6|   Sabrina|   Conway| England|Birmingham|1963-06-25|\n|         7|Jacqueline|Zimmerman| Belgium|  Brussels|2003-05-04|\n|         8|     Dewey|  Francis| England| Liverpool|1957-01-10|\n|         9|     Haley| McDowell| Belgium|  Brussels|1987-04-25|\n|        10|  Brittany|     Tate| England| Liverpool|1991-04-07|\n|        11|     Dario|    Morse|Portugal|     Porto|1967-06-02|\n|        12|      Josh|   Conrad| England| Liverpool|1983-10-18|\n|        13|   Coleman|     Rios| England|Birmingham|1951-01-17|\n|        14|     Shari|   Nieves| England| Liverpool|1983-05-04|\n|        15|      Keri|    Reyes| England|    London|1996-02-09|\n|        16|      Lacy|    Cross| Belgium|   Antwerp|1998-04-03|\n|        17|    Willie|   Bonner| England| Liverpool|1950-08-02|\n|        18|    Silvia|  Justice| England| Liverpool|1966-02-03|\n|        19|      Jody|   Vinson| Belgium|  Brussels|1961-04-06|\n|        20|  Clifford|     Kent| England| Liverpool|1991-11-14|\n+----------+----------+---------+--------+----------+----------+\nonly showing top 20 rows\n\n"]}],"execution_count":4,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"73a7b914-2941-4c49-83ec-56907cf1039e"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_invoices = StructType([\n","    StructField(\"InvoiceId\", LongType() ),\n","    StructField(\"Date\", DateType() ),\n","    StructField(\"TotalSalesAmount\", FloatType() ),\n","    StructField(\"CustomerId\", LongType() )\n","])\n","\n","df_invoices = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_invoices)\n","         .option(\"dateFormat\", \"MM/dd/yyyy\")\n","         .option(\"inferSchema\", \"true\") \n","         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n",")\n","\n","df_invoices.printSchema()\n","df_invoices.show()\n","\n","( df_invoices.write\n","             .mode(\"overwrite\")\n","             .option(\"overwriteSchema\", \"True\")\n","             .format(\"delta\")\n","             .save(\"Tables/silver_invoices\")\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T21:52:22.8808944Z","session_start_time":null,"execution_start_time":"2023-10-10T21:52:23.3014564Z","execution_finish_time":"2023-10-10T21:52:28.3912137Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4368,"rowCount":50,"usageDescription":"","jobId":35,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:52:27.240GMT","completionTime":"2023-10-10T21:52:27.314GMT","stageIds":[51,49,50],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4368,"dataRead":1581,"rowCount":54,"usageDescription":"","jobId":34,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:52:26.464GMT","completionTime":"2023-10-10T21:52:27.207GMT","stageIds":[48,47],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1581,"dataRead":1525,"rowCount":8,"usageDescription":"","jobId":33,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:52:26.234GMT","completionTime":"2023-10-10T21:52:26.310GMT","stageIds":[46],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":32,"name":"","description":"Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n)","submissionTime":"2023-10-10T21:52:25.778GMT","completionTime":"2023-10-10T21:52:25.778GMT","stageIds":[],"jobGroup":"7","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5836276,"dataRead":8180396,"rowCount":1241294,"usageDescription":"","jobId":31,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n)","submissionTime":"2023-10-10T21:52:24.590GMT","completionTime":"2023-10-10T21:52:25.631GMT","stageIds":[45,44],"jobGroup":"7","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":8180396,"dataRead":18249137,"rowCount":1241294,"usageDescription":"","jobId":30,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n)","submissionTime":"2023-10-10T21:52:23.965GMT","completionTime":"2023-10-10T21:52:24.545GMT","stageIds":[43],"jobGroup":"7","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":21,"usageDescription":"","jobId":29,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()\n\n( df_invoices.write\n             .mode(\"overwrite\")\n             .option(\"overwriteSchema\", \"True\")\n             .format(\"delta\")\n             .save(\"Tables/silver_invoices\")\n)","submissionTime":"2023-10-10T21:52:23.498GMT","completionTime":"2023-10-10T21:52:23.701GMT","stageIds":[42],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"430e9b62-51b6-4f0f-bb70-691cce8234e5"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 7, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- InvoiceId: long (nullable = true)\n |-- Date: date (nullable = true)\n |-- TotalSalesAmount: float (nullable = true)\n |-- CustomerId: long (nullable = true)\n\n+---------+----------+----------------+----------+\n|InvoiceId|      Date|TotalSalesAmount|CustomerId|\n+---------+----------+----------------+----------+\n|        1|2020-01-01|            72.0|         1|\n|        2|2020-01-01|            35.0|         2|\n|        3|2020-01-01|             5.0|         3|\n|        4|2020-01-01|           58.75|         4|\n|        5|2020-01-01|            39.1|         5|\n|        6|2020-01-01|            37.5|         6|\n|        7|2020-01-01|           22.15|         7|\n|        8|2020-01-01|            58.1|         8|\n|        9|2020-01-01|           51.75|         9|\n|       10|2020-01-01|          120.75|        10|\n|       11|2020-01-02|           78.95|        11|\n|       12|2020-01-02|            54.1|        12|\n|       13|2020-01-02|             7.5|        13|\n|       14|2020-01-02|            49.2|        14|\n|       15|2020-01-02|            2.85|        15|\n|       16|2020-01-02|             2.5|        16|\n|       17|2020-01-02|            24.0|        17|\n|       18|2020-01-02|            34.4|        18|\n|       19|2020-01-02|            79.6|        19|\n|       20|2020-01-02|           31.25|        20|\n+---------+----------+----------------+----------+\nonly showing top 20 rows\n\n"]}],"execution_count":5,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"37d1b67b-1893-446f-926a-7f84b5774a81"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_invoice_details = StructType([\n","    StructField(\"Id\", LongType() ),\n","    StructField(\"Quantity\", LongType() ),\n","    StructField(\"SalesAmount\", FloatType() ),\n","    StructField(\"InvoiceId\", LongType() ),\n","    StructField(\"ProductId\", LongType() )\n","])\n","\n","df_invoice_details = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_invoice_details)\n","         .option(\"dateFormat\", \"MM/dd/yyyy\")\n","         .option(\"inferSchema\", \"true\") \n","         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n",")\n","\n","df_invoice_details.printSchema()\n","df_invoice_details.show()\n","\n","( df_invoice_details.write\n","                    .mode(\"overwrite\")\n","                    .option(\"overwriteSchema\", \"True\")\n","                    .format(\"delta\")\n","                    .save(\"Tables/silver_invoice_details\")\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T21:54:57.5864275Z","session_start_time":null,"execution_start_time":"2023-10-10T21:54:57.9566962Z","execution_finish_time":"2023-10-10T21:55:03.0031631Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4362,"rowCount":50,"usageDescription":"","jobId":42,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:55:02.166GMT","completionTime":"2023-10-10T21:55:02.204GMT","stageIds":[60,61,59],"jobGroup":"8","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4362,"dataRead":1593,"rowCount":54,"usageDescription":"","jobId":41,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:55:01.434GMT","completionTime":"2023-10-10T21:55:02.136GMT","stageIds":[57,58],"jobGroup":"8","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1593,"dataRead":1595,"rowCount":8,"usageDescription":"","jobId":40,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:55:01.222GMT","completionTime":"2023-10-10T21:55:01.294GMT","stageIds":[56],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":39,"name":"","description":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n)","submissionTime":"2023-10-10T21:55:00.710GMT","completionTime":"2023-10-10T21:55:00.710GMT","stageIds":[],"jobGroup":"8","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":11187276,"dataRead":19900346,"rowCount":2483206,"usageDescription":"","jobId":38,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n)","submissionTime":"2023-10-10T21:54:59.272GMT","completionTime":"2023-10-10T21:55:00.550GMT","stageIds":[54,55],"jobGroup":"8","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":19900346,"dataRead":28335549,"rowCount":2483206,"usageDescription":"","jobId":37,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n)","submissionTime":"2023-10-10T21:54:58.538GMT","completionTime":"2023-10-10T21:54:59.231GMT","stageIds":[53],"jobGroup":"8","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":21,"usageDescription":"","jobId":36,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/bronze_landing_zone/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()\n\n( df_invoice_details.write\n                    .mode(\"overwrite\")\n                    .option(\"overwriteSchema\", \"True\")\n                    .format(\"delta\")\n                    .save(\"Tables/silver_invoice_details\")\n)","submissionTime":"2023-10-10T21:54:58.190GMT","completionTime":"2023-10-10T21:54:58.347GMT","stageIds":[52],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"77adc314-c0e9-453a-b500-749278fe8198"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 8, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- Id: long (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- SalesAmount: float (nullable = true)\n |-- InvoiceId: long (nullable = true)\n |-- ProductId: long (nullable = true)\n\n+---+--------+-----------+---------+---------+\n| Id|Quantity|SalesAmount|InvoiceId|ProductId|\n+---+--------+-----------+---------+---------+\n|  2|       2|        4.5|        1|        5|\n|  2|      30|       67.5|        1|        9|\n|  3|      20|       35.0|        2|        7|\n|  4|       4|        5.0|        3|        3|\n|  5|      20|       25.0|        4|        6|\n|  5|       6|       10.5|        4|        7|\n|  5|      31|      23.25|        4|        1|\n|  6|       3|       2.85|        5|        2|\n|  6|      29|      36.25|        5|        3|\n|  7|      15|       37.5|        6|        8|\n|  8|      17|      16.15|        7|        4|\n|  8|       8|        6.0|        7|        1|\n|  9|      28|       26.6|        8|        2|\n|  9|      18|       31.5|        8|        7|\n| 10|      25|      23.75|        9|        4|\n| 10|      16|       28.0|        9|        7|\n| 11|       7|      12.25|       10|        7|\n| 11|      33|      57.75|       10|        7|\n| 11|      29|      50.75|       10|        7|\n| 12|      31|      54.25|       11|        7|\n+---+--------+-----------+---------+---------+\nonly showing top 20 rows\n\n"]}],"execution_count":6,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4cf1db35-9943-4e70-ba42-fa48cd656c95"},{"cell_type":"code","source":["# create silver layer products table\n","df_gold_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\n","df_gold_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n","df_gold_products.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T21:55:38.0107649Z","session_start_time":null,"execution_start_time":"2023-10-10T21:55:38.3457409Z","execution_finish_time":"2023-10-10T21:55:43.2905359Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":1444,"rowCount":10,"usageDescription":"","jobId":51,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show()","submissionTime":"2023-10-10T21:55:42.032GMT","completionTime":"2023-10-10T21:55:42.146GMT","stageIds":[75],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1731,"rowCount":3,"usageDescription":"","jobId":50,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show(): Filtering files for query","submissionTime":"2023-10-10T21:55:41.178GMT","completionTime":"2023-10-10T21:55:41.942GMT","stageIds":[74,73],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4329,"rowCount":50,"usageDescription":"","jobId":49,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show(): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:55:40.803GMT","completionTime":"2023-10-10T21:55:40.850GMT","stageIds":[70,71,72],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4329,"dataRead":1485,"rowCount":54,"usageDescription":"","jobId":48,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show(): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:55:40.128GMT","completionTime":"2023-10-10T21:55:40.777GMT","stageIds":[68,69],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1485,"dataRead":1364,"rowCount":8,"usageDescription":"","jobId":47,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show(): Compute snapshot for version: 0","submissionTime":"2023-10-10T21:55:39.868GMT","completionTime":"2023-10-10T21:55:39.950GMT","stageIds":[67],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show()","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":46,"name":"","description":"Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show()","submissionTime":"2023-10-10T21:55:39.328GMT","completionTime":"2023-10-10T21:55:39.328GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1874,"dataRead":338,"rowCount":20,"usageDescription":"","jobId":45,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show()","submissionTime":"2023-10-10T21:55:39.009GMT","completionTime":"2023-10-10T21:55:39.234GMT","stageIds":[66,65],"jobGroup":"9","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":338,"dataRead":1444,"rowCount":20,"usageDescription":"","jobId":44,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show()","submissionTime":"2023-10-10T21:55:38.877GMT","completionTime":"2023-10-10T21:55:38.970GMT","stageIds":[64],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1731,"rowCount":3,"usageDescription":"","jobId":43,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 9:\n# create silver layer products table\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/silver_products\")\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\ndf_silver_products.show(): Filtering files for query","submissionTime":"2023-10-10T21:55:38.673GMT","completionTime":"2023-10-10T21:55:38.823GMT","stageIds":[63,62],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"82e5e309-4ffb-4ebb-beb6-2be2a6bf5164"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 9, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+---------+----------+\n|ProductId|  Product|  Category|\n+---------+---------+----------+\n|        1|   Apples|    Fruits|\n|        2|  Bananas|    Fruits|\n|        3|  Oranges|    Fruits|\n|        4|  Carrots|Vegetables|\n|        5|Cucumbers|Vegetables|\n|        6| Potatoes|Vegetables|\n|        7| Tomatoes|Vegetables|\n|        8|     Milk|     Dairy|\n|        9|   Butter|     Dairy|\n|       10|   Cheese|     Dairy|\n+---------+---------+----------+\n\n"]}],"execution_count":7,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"deae8c98-52fd-4f2a-aad9-2cef0bc118f9"},{"cell_type":"code","source":["# create silver layer customers table\n","from pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n","\n","df_gold_customers = (\n","    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n","            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n","            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n","            .drop('FirstName', 'LastName')\n",")\n","\n","df_gold_customers.printSchema()\n","df_gold_customers.show()\n","\n","( df_gold_customers.write\n","                   .mode(\"overwrite\")\n","                   .option(\"overwriteSchema\", \"True\")\n","                   .format(\"delta\")\n","                   .save(\"Tables/customers\")\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T22:10:25.3848141Z","session_start_time":null,"execution_start_time":"2023-10-10T22:10:25.7307374Z","execution_finish_time":"2023-10-10T22:10:30.7642859Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4368,"rowCount":50,"usageDescription":"","jobId":105,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T22:10:29.399GMT","completionTime":"2023-10-10T22:10:29.433GMT","stageIds":[157,158,159],"jobGroup":"14","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4368,"dataRead":1640,"rowCount":54,"usageDescription":"","jobId":104,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T22:10:28.898GMT","completionTime":"2023-10-10T22:10:29.379GMT","stageIds":[155,156],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1640,"dataRead":1759,"rowCount":8,"usageDescription":"","jobId":103,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n): Compute snapshot for version: 0","submissionTime":"2023-10-10T22:10:28.673GMT","completionTime":"2023-10-10T22:10:28.740GMT","stageIds":[154],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":102,"name":"","description":"Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n)","submissionTime":"2023-10-10T22:10:28.249GMT","completionTime":"2023-10-10T22:10:28.249GMT","stageIds":[],"jobGroup":"14","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":4211436,"dataRead":6713610,"rowCount":434754,"usageDescription":"","jobId":101,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n)","submissionTime":"2023-10-10T22:10:27.421GMT","completionTime":"2023-10-10T22:10:28.132GMT","stageIds":[153,152],"jobGroup":"14","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":6713610,"dataRead":2018191,"rowCount":434754,"usageDescription":"","jobId":100,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n)","submissionTime":"2023-10-10T22:10:27.062GMT","completionTime":"2023-10-10T22:10:27.378GMT","stageIds":[151],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1879,"rowCount":3,"usageDescription":"","jobId":99,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n): Filtering files for query","submissionTime":"2023-10-10T22:10:26.864GMT","completionTime":"2023-10-10T22:10:26.994GMT","stageIds":[150,149],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":2018191,"rowCount":4096,"usageDescription":"","jobId":98,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n)","submissionTime":"2023-10-10T22:10:26.325GMT","completionTime":"2023-10-10T22:10:26.536GMT","stageIds":[148],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1879,"rowCount":3,"usageDescription":"","jobId":97,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\n# create silver layer customers table\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_gold_customers = (\n    spark.read.format(\"delta\").load(\"Tables/silver_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_gold_customers.printSchema()\ndf_gold_customers.show()\n\n( df_gold_customers.write\n                   .mode(\"overwrite\")\n                   .option(\"overwriteSchema\", \"True\")\n                   .format(\"delta\")\n                   .save(\"Tables/customers\")\n): Filtering files for query","submissionTime":"2023-10-10T22:10:25.968GMT","completionTime":"2023-10-10T22:10:26.260GMT","stageIds":[147,146],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ad0514c6-0de2-4d52-8184-1dfb02a73f6e"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 14, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- CustomerId: long (nullable = true)\n |-- Country: string (nullable = true)\n |-- City: string (nullable = true)\n |-- DOB: date (nullable = true)\n |-- Customer: string (nullable = false)\n |-- Age: long (nullable = true)\n\n+----------+-------+-------+----------+-----------------+---+\n|CustomerId|Country|   City|       DOB|         Customer|Age|\n+----------+-------+-------+----------+-----------------+---+\n|        16|Belgium|Antwerp|1998-04-03|       Lacy Cross| 25|\n|        57|Belgium|Antwerp|1992-04-02|     Cheryl Morse| 31|\n|        77|Belgium|Antwerp|1970-09-17|      Dolly Boyer| 53|\n|       133|Belgium|Antwerp|1991-06-13| Alyssa Frederick| 32|\n|       137|Belgium|Antwerp|1992-08-17|   Maxine Walters| 31|\n|       141|Belgium|Antwerp|1951-06-06|    Burl McCarthy| 72|\n|       147|Belgium|Antwerp|1983-11-20|     Eloise Bruce| 39|\n|       184|Belgium|Antwerp|2004-10-23|   Gwen Whitehead| 18|\n|       211|Belgium|Antwerp|1952-03-01|    Prince Franco| 71|\n|       221|Belgium|Antwerp|1944-02-21|  Guadalupe Lewis| 79|\n|       232|Belgium|Antwerp|1956-08-13|      Willie Pate| 67|\n|       268|Belgium|Antwerp|1963-06-17|     Graham Klein| 60|\n|       275|Belgium|Antwerp|1973-06-14|Houston Alexander| 50|\n|       300|Belgium|Antwerp|1945-07-07|    Lindsay Pitts| 78|\n|       319|Belgium|Antwerp|2002-02-22|         Elsa Cox| 21|\n|       329|Belgium|Antwerp|1989-03-20|   Rosanna Hodges| 34|\n|       359|Belgium|Antwerp|1999-08-09|    Hunter Newton| 24|\n|       413|Belgium|Antwerp|1966-01-06|     Alana Graves| 57|\n|       426|Belgium|Antwerp|1960-07-19|     Jamie Gibson| 63|\n|       427|Belgium|Antwerp|1957-11-19| Garrett Mitchell| 65|\n+----------+-------+-------+----------+-----------------+---+\nonly showing top 20 rows\n\n"]}],"execution_count":12,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"87a11595-0a6c-4b16-aa8d-8883217810e0"},{"cell_type":"code","source":["# create silver layer sales table\n","from pyspark.sql.functions import col, desc, concat, lit, floor, datediff\n","from pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n","\n","df_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\n","df_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n","\n","df_gold_sales = (\n","    df_silver_invoice_details\n","            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n","            .withColumnRenamed('SalesAmount', 'Sales')\n","            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n","                               (month(col('Date'))*100) + \n","                               (dayofmonth(col('Date')))   )\n","\n","            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n","            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n",")\n","\n","df_gold_sales.printSchema()\n","df_gold_sales.show()\n","\n","( df_gold_sales.write\n","               .mode(\"overwrite\")\n","               .option(\"overwriteSchema\", \"True\")\n","               .format(\"delta\")\n","               .save(\"Tables/sales\")\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T22:09:44.0593132Z","session_start_time":null,"execution_start_time":"2023-10-10T22:09:44.4298259Z","execution_finish_time":"2023-10-10T22:09:52.734467Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":12,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4387,"rowCount":50,"usageDescription":"","jobId":96,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Compute snapshot for version: 1","submissionTime":"2023-10-10T22:09:51.498GMT","completionTime":"2023-10-10T22:09:51.547GMT","stageIds":[143,144,145],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4387,"dataRead":3263,"rowCount":57,"usageDescription":"","jobId":95,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Compute snapshot for version: 1","submissionTime":"2023-10-10T22:09:50.860GMT","completionTime":"2023-10-10T22:09:51.467GMT","stageIds":[141,142],"jobGroup":"13","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3263,"dataRead":2982,"rowCount":14,"usageDescription":"","jobId":94,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Compute snapshot for version: 1","submissionTime":"2023-10-10T22:09:50.664GMT","completionTime":"2023-10-10T22:09:50.726GMT","stageIds":[140],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1848,"rowCount":3,"usageDescription":"","jobId":93,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...","submissionTime":"2023-10-10T22:09:50.031GMT","completionTime":"2023-10-10T22:09:50.176GMT","stageIds":[139,138],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5538821,"dataRead":10895638,"rowCount":2483206,"usageDescription":"","jobId":92,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...","submissionTime":"2023-10-10T22:09:48.843GMT","completionTime":"2023-10-10T22:09:49.901GMT","stageIds":[136,137],"jobGroup":"13","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":10895638,"dataRead":5598704,"rowCount":2483206,"usageDescription":"","jobId":91,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...","submissionTime":"2023-10-10T22:09:47.722GMT","completionTime":"2023-10-10T22:09:48.804GMT","stageIds":[135],"jobGroup":"13","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":89,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Filtering files for query","submissionTime":"2023-10-10T22:09:46.661GMT","completionTime":"2023-10-10T22:09:46.826GMT","stageIds":[132,133],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":88,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Filtering files for query","submissionTime":"2023-10-10T22:09:46.420GMT","completionTime":"2023-10-10T22:09:46.557GMT","stageIds":[130,131],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":5597229,"rowCount":4096,"usageDescription":"","jobId":87,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...","submissionTime":"2023-10-10T22:09:46.043GMT","completionTime":"2023-10-10T22:09:46.237GMT","stageIds":[129],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":1475,"rowCount":0,"usageDescription":"","jobId":86,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...","submissionTime":"2023-10-10T22:09:45.869GMT","completionTime":"2023-10-10T22:09:46.038GMT","stageIds":[128],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":84,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Filtering files for query","submissionTime":"2023-10-10T22:09:44.910GMT","completionTime":"2023-10-10T22:09:45.076GMT","stageIds":[125,126],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":83,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# create silver layer sales table\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\ndf_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\n\ndf_gold_sales = (\n    df_silver_invoice_details\n            .join(df_silver_invoices, df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_gold_sales.printSchema()\ndf_gold_sales.show()\n\n( df_gold_sales.write\n               ...: Filtering files for query","submissionTime":"2023-10-10T22:09:44.665GMT","completionTime":"2023-10-10T22:09:44.824GMT","stageIds":[123,124],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"4b7462f0-92dc-408c-a40f-6be54f3f4dcc"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 13, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- Date: date (nullable = true)\n |-- DateKey: integer (nullable = true)\n |-- CustomerId: long (nullable = true)\n |-- ProductId: long (nullable = true)\n |-- Sales: float (nullable = true)\n |-- Quantity: long (nullable = true)\n\n+----------+--------+----------+---------+-----+--------+\n|      Date| DateKey|CustomerId|ProductId|Sales|Quantity|\n+----------+--------+----------+---------+-----+--------+\n|2020-02-24|20200224|       606|       10| 90.0|      24|\n|2020-03-06|20200306|       776|       10| 90.0|      24|\n|2020-03-06|20200306|       789|       10| 90.0|      24|\n|2020-03-07|20200307|       803|       10| 90.0|      24|\n|2020-03-18|20200318|      1079|       10| 90.0|      24|\n|2020-04-02|20200402|      1663|       10| 90.0|      24|\n|2020-04-11|20200411|      1992|       10| 90.0|      24|\n|2020-04-12|20200412|      2079|       10| 90.0|      24|\n|2020-04-14|20200414|      2244|       10| 90.0|      24|\n|2020-04-29|20200429|      3055|       10| 90.0|      24|\n|2020-05-03|20200503|      3389|       10| 90.0|      24|\n|2020-05-07|20200507|      3633|       10| 90.0|      24|\n|2020-05-09|20200509|      3773|       10| 90.0|      24|\n|2020-05-11|20200511|      3823|       10| 90.0|      24|\n|2020-05-11|20200511|      3828|       10| 90.0|      24|\n|2020-05-13|20200513|      4036|       10| 90.0|      24|\n|2020-05-19|20200519|      4407|       10| 90.0|      24|\n|2020-05-21|20200521|      4551|       10| 90.0|      24|\n|2020-05-23|20200523|      4638|       10| 90.0|      24|\n|2020-05-26|20200526|      4774|       10| 90.0|      24|\n+----------+--------+----------+---------+-----+--------+\nonly showing top 20 rows\n\n"]}],"execution_count":11,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f98476ca-520e-441b-94e9-734b9f4e8e29"},{"cell_type":"code","source":["# create gold layer calendar table \n","from datetime import date\n","import pandas as pd\n","from pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n","\n","first_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\n","last_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n","\n","start_date = date(first_sales_date.year, 1, 1)\n","end_date = date(last_sales_date.year, 12, 31)\n","\n","df_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n","\n","df_calendar_spark = (\n","     spark.createDataFrame(df_calendar_ps)\n","       .withColumnRenamed(\"0\", \"timestamp\")\n","       .withColumn(\"Date\", to_date(col('timestamp')))\n","       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n","                              (month(col('timestamp'))*100) + \n","                              (dayofmonth(col('timestamp')))   )\n","       .withColumn(\"Year\", year(col('timestamp'))  )\n","       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n","       .withColumn(\"Month\", date_format(col('timestamp'),'yyyy-MM')  )\n","       .withColumn(\"Day\", dayofmonth(col('timestamp'))  )\n","       .withColumn(\"MonthInYear\", date_format(col('timestamp'),'MMMM')  )\n","       .withColumn(\"MonthInYearSort\", month(col('timestamp'))  )\n","       .withColumn(\"DayOfWeek\", date_format(col('timestamp'),'EEEE')  )\n","       .withColumn(\"DayOfWeekSort\", dayofweek(col('timestamp')))\n","       .drop('timestamp')\n",")\n","\n","df_calendar_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/calendar\")\n","df_calendar_spark.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"56b70a48-ce33-45b2-95f5-2856c35aef71","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-10T22:06:34.8772917Z","session_start_time":null,"execution_start_time":"2023-10-10T22:06:35.252936Z","execution_finish_time":"2023-10-10T22:06:41.8173429Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":15,"UNKNOWN":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":82,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:41.536GMT","completionTime":"2023-10-10T22:06:41.573GMT","stageIds":[122],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4416,"rowCount":50,"usageDescription":"","jobId":81,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Compute snapshot for version: 0","submissionTime":"2023-10-10T22:06:41.425GMT","completionTime":"2023-10-10T22:06:41.464GMT","stageIds":[121,119,120],"jobGroup":"12","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4416,"dataRead":1766,"rowCount":54,"usageDescription":"","jobId":80,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Compute snapshot for version: 0","submissionTime":"2023-10-10T22:06:40.901GMT","completionTime":"2023-10-10T22:06:41.405GMT","stageIds":[117,118],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1766,"dataRead":2315,"rowCount":8,"usageDescription":"","jobId":79,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Compute snapshot for version: 0","submissionTime":"2023-10-10T22:06:40.612GMT","completionTime":"2023-10-10T22:06:40.712GMT","stageIds":[116],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":78,"name":"","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:40.144GMT","completionTime":"2023-10-10T22:06:40.144GMT","stageIds":[],"jobGroup":"12","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":22946,"dataRead":43264,"rowCount":2922,"usageDescription":"","jobId":77,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:39.719GMT","completionTime":"2023-10-10T22:06:40.005GMT","stageIds":[114,115],"jobGroup":"12","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":43264,"dataRead":0,"rowCount":1461,"usageDescription":"","jobId":76,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:39.586GMT","completionTime":"2023-10-10T22:06:39.675GMT","stageIds":[113],"jobGroup":"12","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:7","dataWritten":0,"dataRead":173,"rowCount":3,"usageDescription":"","jobId":75,"name":"collect at /tmp/ipykernel_8279/361292844.py:7","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:38.905GMT","completionTime":"2023-10-10T22:06:38.936GMT","stageIds":[111,112],"jobGroup":"12","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:7","dataWritten":173,"dataRead":5595219,"rowCount":1241606,"usageDescription":"","jobId":74,"name":"collect at /tmp/ipykernel_8279/361292844.py:7","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:38.388GMT","completionTime":"2023-10-10T22:06:38.877GMT","stageIds":[110],"jobGroup":"12","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:7","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":72,"name":"collect at /tmp/ipykernel_8279/361292844.py:7","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Filtering files for query","submissionTime":"2023-10-10T22:06:37.529GMT","completionTime":"2023-10-10T22:06:37.797GMT","stageIds":[107,108],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:7","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":71,"name":"collect at /tmp/ipykernel_8279/361292844.py:7","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Filtering files for query","submissionTime":"2023-10-10T22:06:37.232GMT","completionTime":"2023-10-10T22:06:37.392GMT","stageIds":[105,106],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:6","dataWritten":0,"dataRead":173,"rowCount":3,"usageDescription":"","jobId":70,"name":"collect at /tmp/ipykernel_8279/361292844.py:6","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:37.060GMT","completionTime":"2023-10-10T22:06:37.089GMT","stageIds":[103,104],"jobGroup":"12","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:6","dataWritten":173,"dataRead":5595219,"rowCount":1241606,"usageDescription":"","jobId":69,"name":"collect at /tmp/ipykernel_8279/361292844.py:6","description":"Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...","submissionTime":"2023-10-10T22:06:36.599GMT","completionTime":"2023-10-10T22:06:37.034GMT","stageIds":[102],"jobGroup":"12","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:6","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":67,"name":"collect at /tmp/ipykernel_8279/361292844.py:6","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Filtering files for query","submissionTime":"2023-10-10T22:06:35.721GMT","completionTime":"2023-10-10T22:06:35.848GMT","stageIds":[99,100],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8279/361292844.py:6","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":66,"name":"collect at /tmp/ipykernel_8279/361292844.py:6","description":"Delta: Job group for statement 12:\n# create gold layer calendar table \nfrom datetime import date\nimport pandas as pd\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n\nfirst_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\n#os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n      ...: Filtering files for query","submissionTime":"2023-10-10T22:06:35.461GMT","completionTime":"2023-10-10T22:06:35.632GMT","stageIds":[97,98],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"417005eb-a94d-49f0-92f4-eeb4ca8a42dc"},"text/plain":"StatementMeta(, 56b70a48-ce33-45b2-95f5-2856c35aef71, 12, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"]},{"output_type":"stream","name":"stdout","text":["+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n|      Date| DateKey|Year|Quarter|  Month|Day|MonthInYear|MonthInYearSort|DayOfWeek|DayOfWeekSort|\n+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n|2020-01-01|20200101|2020|2020-01|2020-01|  1|    January|              1|Wednesday|            4|\n|2020-01-02|20200102|2020|2020-01|2020-01|  2|    January|              1| Thursday|            5|\n|2020-01-03|20200103|2020|2020-01|2020-01|  3|    January|              1|   Friday|            6|\n|2020-01-04|20200104|2020|2020-01|2020-01|  4|    January|              1| Saturday|            7|\n|2020-01-05|20200105|2020|2020-01|2020-01|  5|    January|              1|   Sunday|            1|\n|2020-01-06|20200106|2020|2020-01|2020-01|  6|    January|              1|   Monday|            2|\n|2020-01-07|20200107|2020|2020-01|2020-01|  7|    January|              1|  Tuesday|            3|\n|2020-01-08|20200108|2020|2020-01|2020-01|  8|    January|              1|Wednesday|            4|\n|2020-01-09|20200109|2020|2020-01|2020-01|  9|    January|              1| Thursday|            5|\n|2020-01-10|20200110|2020|2020-01|2020-01| 10|    January|              1|   Friday|            6|\n|2020-01-11|20200111|2020|2020-01|2020-01| 11|    January|              1| Saturday|            7|\n|2020-01-12|20200112|2020|2020-01|2020-01| 12|    January|              1|   Sunday|            1|\n|2020-01-13|20200113|2020|2020-01|2020-01| 13|    January|              1|   Monday|            2|\n|2020-01-14|20200114|2020|2020-01|2020-01| 14|    January|              1|  Tuesday|            3|\n|2020-01-15|20200115|2020|2020-01|2020-01| 15|    January|              1|Wednesday|            4|\n|2020-01-16|20200116|2020|2020-01|2020-01| 16|    January|              1| Thursday|            5|\n|2020-01-17|20200117|2020|2020-01|2020-01| 17|    January|              1|   Friday|            6|\n|2020-01-18|20200118|2020|2020-01|2020-01| 18|    January|              1| Saturday|            7|\n|2020-01-19|20200119|2020|2020-01|2020-01| 19|    January|              1|   Sunday|            1|\n|2020-01-20|20200120|2020|2020-01|2020-01| 20|    January|              1|   Monday|            2|\n+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\nonly showing top 20 rows\n\n"]}],"execution_count":10,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"58438ae5-1d64-489c-9d0c-44edece5b09b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}