{"cells":[{"cell_type":"code","source":["import requests\n","\n","csv_base_url = \"https://github.com/PowerBiDevCamp/Python-In-Fabric-Notebooks/raw/main/ProductSalesData/\"\n","\n","csv_files = { \"Customers.csv\", \"Products.csv\", \"Invoices.csv\", \"InvoiceDetails.csv\" }\n","\n","folder_path = \"Files/landing_zone_sales/\"\n","\n","for csv_file in csv_files:\n","    csv_file_path = csv_base_url + csv_file\n","    with requests.get(csv_file_path) as response:\n","        csv_content = response.content.decode('utf-8-sig')\n","        mssparkutils.fs.put(folder_path + csv_file, csv_content, True)\n","        print(csv_file + \" copied to Lakehouse file in OneLake\")"],"outputs":[],"execution_count":null,"metadata":{},"id":"b8105860-3a06-4b08-bcf1-cfcf4f18ed63"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_products = StructType([\n","    StructField(\"ProductId\", LongType() ),\n","    StructField(\"Product\", StringType() ),\n","    StructField(\"Category\", StringType() )\n","])\n","\n","df_products = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_products)\n","         .load(\"Files/landing_zone_sales/Products.csv\")\n",")\n","\n","df_products.printSchema()\n","df_products.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:49:01.5864342Z","session_start_time":null,"execution_start_time":"2023-10-07T21:49:01.9190493Z","execution_finish_time":"2023-10-07T21:49:03.5986166Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":1,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":209,"rowCount":10,"usageDescription":"","jobId":8,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 4:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_products = StructType([\n    StructField(\"ProductId\", LongType() ),\n    StructField(\"Product\", StringType() ),\n    StructField(\"Category\", StringType() )\n])\n\ndf_products = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_products)\n         .load(\"Files/landing_zone_sales/Products.csv\")\n)\n\ndf_products.printSchema()\ndf_products.show()","submissionTime":"2023-10-07T21:49:02.270GMT","completionTime":"2023-10-07T21:49:02.744GMT","stageIds":[12],"jobGroup":"4","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"33731951-f868-4157-a070-7fc51740c0d2"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- ProductId: long (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n\n+---------+---------+----------+\n|ProductId|  Product|  Category|\n+---------+---------+----------+\n|        1|   Apples|    Fruits|\n|        2|  Bananas|    Fruits|\n|        3|  Oranges|    Fruits|\n|        4|  Carrots|Vegetables|\n|        5|Cucumbers|Vegetables|\n|        6| Potatoes|Vegetables|\n|        7| Tomatoes|Vegetables|\n|        8|     Milk|     Dairy|\n|        9|   Butter|     Dairy|\n|       10|   Cheese|     Dairy|\n+---------+---------+----------+\n\n"]}],"execution_count":2,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"673d17b9-74d6-4c2e-90db-9e377a1853e2"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined with StructType and StructField \n","schema_customers = StructType([\n","    StructField(\"CustomerId\", LongType() ),\n","    StructField(\"FirstName\", StringType() ),\n","    StructField(\"LastName\", StringType() ),\n","    StructField(\"Country\", StringType() ),\n","    StructField(\"City\", StringType() ),\n","    StructField(\"DOB\", DateType() ),\n","])\n","\n","df_customers = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_customers)\n","         .option(\"dateFormat\", \"M/d/yyyy\")\n","         .option(\"inferSchema\", \"true\")\n","         .load(\"Files/landing_zone_sales/Customers.csv\")\n",")\n","\n","df_customers.printSchema()\n","df_customers.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:49:07.9156324Z","session_start_time":null,"execution_start_time":"2023-10-07T21:49:08.2160689Z","execution_finish_time":"2023-10-07T21:49:09.0760894Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":1,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":21,"usageDescription":"","jobId":9,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined with StructType and StructField \nschema_customers = StructType([\n    StructField(\"CustomerId\", LongType() ),\n    StructField(\"FirstName\", StringType() ),\n    StructField(\"LastName\", StringType() ),\n    StructField(\"Country\", StringType() ),\n    StructField(\"City\", StringType() ),\n    StructField(\"DOB\", DateType() ),\n])\n\ndf_customers = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_customers)\n         .option(\"dateFormat\", \"M/d/yyyy\")\n         .option(\"inferSchema\", \"true\")\n         .load(\"Files/landing_zone_sales/Customers.csv\")\n)\n\ndf_customers.printSchema()\ndf_customers.show()","submissionTime":"2023-10-07T21:49:08.324GMT","completionTime":"2023-10-07T21:49:08.590GMT","stageIds":[13],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"bda3d72c-ec61-46eb-95b1-afed50c21259"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- CustomerId: long (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- City: string (nullable = true)\n |-- DOB: date (nullable = true)\n\n+----------+----------+---------+--------+----------+----------+\n|CustomerId| FirstName| LastName| Country|      City|       DOB|\n+----------+----------+---------+--------+----------+----------+\n|         1|   Yolanda|   Wagner| Belgium|  Brussels|1970-02-24|\n|         2|    Arnold|   Harmon| England|    London|1943-11-01|\n|         3|      Minh|    Casey| Ireland|  Limerick|1976-11-24|\n|         4|   Russell| McIntyre|Portugal|     Braga|1993-08-05|\n|         5|  Angelina| Santiago| England|    London|1978-08-22|\n|         6|   Sabrina|   Conway| England|Birmingham|1963-06-25|\n|         7|Jacqueline|Zimmerman| Belgium|  Brussels|2003-05-04|\n|         8|     Dewey|  Francis| England| Liverpool|1957-01-10|\n|         9|     Haley| McDowell| Belgium|  Brussels|1987-04-25|\n|        10|  Brittany|     Tate| England| Liverpool|1991-04-07|\n|        11|     Dario|    Morse|Portugal|     Porto|1967-06-02|\n|        12|      Josh|   Conrad| England| Liverpool|1983-10-18|\n|        13|   Coleman|     Rios| England|Birmingham|1951-01-17|\n|        14|     Shari|   Nieves| England| Liverpool|1983-05-04|\n|        15|      Keri|    Reyes| England|    London|1996-02-09|\n|        16|      Lacy|    Cross| Belgium|   Antwerp|1998-04-03|\n|        17|    Willie|   Bonner| England| Liverpool|1950-08-02|\n|        18|    Silvia|  Justice| England| Liverpool|1966-02-03|\n|        19|      Jody|   Vinson| Belgium|  Brussels|1961-04-06|\n|        20|  Clifford|     Kent| England| Liverpool|1991-11-14|\n+----------+----------+---------+--------+----------+----------+\nonly showing top 20 rows\n\n"]}],"execution_count":3,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"73a7b914-2941-4c49-83ec-56907cf1039e"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_invoices = StructType([\n","    StructField(\"InvoiceId\", LongType() ),\n","    StructField(\"Date\", DateType() ),\n","    StructField(\"TotalSalesAmount\", FloatType() ),\n","    StructField(\"CustomerId\", LongType() )\n","])\n","\n","df_invoices = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_invoices)\n","         .option(\"dateFormat\", \"MM/dd/yyyy\")\n","         .option(\"inferSchema\", \"true\") \n","         .load(\"Files/landing_zone_sales/Invoices.csv\")\n",")\n","\n","df_invoices.printSchema()\n","df_invoices.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:49:18.8358891Z","session_start_time":null,"execution_start_time":"2023-10-07T21:49:19.1422634Z","execution_finish_time":"2023-10-07T21:49:20.1139742Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":1,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":21,"usageDescription":"","jobId":10,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoices = StructType([\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"Date\", DateType() ),\n    StructField(\"TotalSalesAmount\", FloatType() ),\n    StructField(\"CustomerId\", LongType() )\n])\n\ndf_invoices = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoices)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/landing_zone_sales/Invoices.csv\")\n)\n\ndf_invoices.printSchema()\ndf_invoices.show()","submissionTime":"2023-10-07T21:49:19.211GMT","completionTime":"2023-10-07T21:49:19.426GMT","stageIds":[14],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"73d364b1-0498-4026-ada1-311a629e1f20"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- InvoiceId: long (nullable = true)\n |-- Date: date (nullable = true)\n |-- TotalSalesAmount: float (nullable = true)\n |-- CustomerId: long (nullable = true)\n\n+---------+----------+----------------+----------+\n|InvoiceId|      Date|TotalSalesAmount|CustomerId|\n+---------+----------+----------------+----------+\n|        1|2020-01-01|            72.0|         1|\n|        2|2020-01-01|            35.0|         2|\n|        3|2020-01-01|             5.0|         3|\n|        4|2020-01-01|           58.75|         4|\n|        5|2020-01-01|            39.1|         5|\n|        6|2020-01-01|            37.5|         6|\n|        7|2020-01-01|           22.15|         7|\n|        8|2020-01-01|            58.1|         8|\n|        9|2020-01-01|           51.75|         9|\n|       10|2020-01-01|          120.75|        10|\n|       11|2020-01-02|           78.95|        11|\n|       12|2020-01-02|            54.1|        12|\n|       13|2020-01-02|             7.5|        13|\n|       14|2020-01-02|            49.2|        14|\n|       15|2020-01-02|            2.85|        15|\n|       16|2020-01-02|             2.5|        16|\n|       17|2020-01-02|            24.0|        17|\n|       18|2020-01-02|            34.4|        18|\n|       19|2020-01-02|            79.6|        19|\n|       20|2020-01-02|           31.25|        20|\n+---------+----------+----------------+----------+\nonly showing top 20 rows\n\n"]}],"execution_count":4,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"37d1b67b-1893-446f-926a-7f84b5774a81"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n","\n","# creating a Spark DataFrame using schema defined using StructType and StructField \n","schema_invoice_details = StructType([\n","    StructField(\"Id\", LongType() ),\n","    StructField(\"Quantity\", LongType() ),\n","    StructField(\"SalesAmount\", FloatType() ),\n","    StructField(\"InvoiceId\", LongType() ),\n","    StructField(\"ProductId\", LongType() )\n","])\n","\n","df_invoice_details = (\n","    spark.read.format(\"csv\")\n","         .option(\"header\",\"true\")\n","         .schema(schema_invoice_details)\n","         .option(\"dateFormat\", \"MM/dd/yyyy\")\n","         .option(\"inferSchema\", \"true\") \n","         .load(\"Files/landing_zone_sales/InvoiceDetails.csv\")\n",")\n","\n","df_invoice_details.printSchema()\n","df_invoice_details.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:49:32.1479974Z","session_start_time":null,"execution_start_time":"2023-10-07T21:49:32.4408145Z","execution_finish_time":"2023-10-07T21:49:33.2901719Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":1,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":65536,"rowCount":21,"usageDescription":"","jobId":12,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, DateType\n\n# creating a Spark DataFrame using schema defined using StructType and StructField \nschema_invoice_details = StructType([\n    StructField(\"Id\", LongType() ),\n    StructField(\"Quantity\", LongType() ),\n    StructField(\"SalesAmount\", FloatType() ),\n    StructField(\"InvoiceId\", LongType() ),\n    StructField(\"ProductId\", LongType() )\n])\n\ndf_invoice_details = (\n    spark.read.format(\"csv\")\n         .option(\"header\",\"true\")\n         .schema(schema_invoice_details)\n         .option(\"dateFormat\", \"MM/dd/yyyy\")\n         .option(\"inferSchema\", \"true\") \n         .load(\"Files/landing_zone_sales/InvoiceDetails.csv\")\n)\n\ndf_invoice_details.printSchema()\ndf_invoice_details.show()","submissionTime":"2023-10-07T21:49:32.486GMT","completionTime":"2023-10-07T21:49:32.639GMT","stageIds":[16],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c8267af5-9e2e-472e-ba26-894cb6486dcd"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 8, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- Id: long (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- SalesAmount: float (nullable = true)\n |-- InvoiceId: long (nullable = true)\n |-- ProductId: long (nullable = true)\n\n+---+--------+-----------+---------+---------+\n| Id|Quantity|SalesAmount|InvoiceId|ProductId|\n+---+--------+-----------+---------+---------+\n|  2|       2|        4.5|        1|        5|\n|  2|      30|       67.5|        1|        9|\n|  3|      20|       35.0|        2|        7|\n|  4|       4|        5.0|        3|        3|\n|  5|      20|       25.0|        4|        6|\n|  5|       6|       10.5|        4|        7|\n|  5|      31|      23.25|        4|        1|\n|  6|       3|       2.85|        5|        2|\n|  6|      29|      36.25|        5|        3|\n|  7|      15|       37.5|        6|        8|\n|  8|      17|      16.15|        7|        4|\n|  8|       8|        6.0|        7|        1|\n|  9|      28|       26.6|        8|        2|\n|  9|      18|       31.5|        8|        7|\n| 10|      25|      23.75|        9|        4|\n| 10|      16|       28.0|        9|        7|\n| 11|       7|      12.25|       10|        7|\n| 11|      33|      57.75|       10|        7|\n| 11|      29|      50.75|       10|        7|\n| 12|      31|      54.25|       11|        7|\n+---+--------+-----------+---------+---------+\nonly showing top 20 rows\n\n"]}],"execution_count":6,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4cf1db35-9943-4e70-ba42-fa48cd656c95"},{"cell_type":"code","source":["# save all bronze layer tables\n","df_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\n","df_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\n","df_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\n","df_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:49:41.2895919Z","session_start_time":null,"execution_start_time":"2023-10-07T21:49:41.5981904Z","execution_finish_time":"2023-10-07T21:49:56.0094299Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":24,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4362,"rowCount":50,"usageDescription":"","jobId":36,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:54.552GMT","completionTime":"2023-10-07T21:49:54.592GMT","stageIds":[51,52,50],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4362,"dataRead":1593,"rowCount":54,"usageDescription":"","jobId":35,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:53.911GMT","completionTime":"2023-10-07T21:49:54.530GMT","stageIds":[48,49],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1593,"dataRead":1595,"rowCount":8,"usageDescription":"","jobId":34,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:53.730GMT","completionTime":"2023-10-07T21:49:53.802GMT","stageIds":[47],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":33,"name":"","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:53.308GMT","completionTime":"2023-10-07T21:49:53.308GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":11187276,"dataRead":19900346,"rowCount":2483206,"usageDescription":"","jobId":32,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:52.025GMT","completionTime":"2023-10-07T21:49:53.219GMT","stageIds":[45,46],"jobGroup":"9","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":19900346,"dataRead":28335549,"rowCount":2483206,"usageDescription":"","jobId":31,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:51.335GMT","completionTime":"2023-10-07T21:49:51.984GMT","stageIds":[44],"jobGroup":"9","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4368,"rowCount":50,"usageDescription":"","jobId":30,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:51.126GMT","completionTime":"2023-10-07T21:49:51.165GMT","stageIds":[42,43,41],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4368,"dataRead":1581,"rowCount":54,"usageDescription":"","jobId":29,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:50.272GMT","completionTime":"2023-10-07T21:49:51.103GMT","stageIds":[39,40],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1581,"dataRead":1525,"rowCount":8,"usageDescription":"","jobId":28,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:49.991GMT","completionTime":"2023-10-07T21:49:50.052GMT","stageIds":[38],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":27,"name":"","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:49.542GMT","completionTime":"2023-10-07T21:49:49.542GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5836276,"dataRead":8180396,"rowCount":1241294,"usageDescription":"","jobId":26,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:48.587GMT","completionTime":"2023-10-07T21:49:49.434GMT","stageIds":[37,36],"jobGroup":"9","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":8180396,"dataRead":18249137,"rowCount":1241294,"usageDescription":"","jobId":25,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:47.835GMT","completionTime":"2023-10-07T21:49:48.540GMT","stageIds":[35],"jobGroup":"9","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4372,"rowCount":50,"usageDescription":"","jobId":24,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:47.623GMT","completionTime":"2023-10-07T21:49:47.658GMT","stageIds":[33,34,32],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4372,"dataRead":1655,"rowCount":54,"usageDescription":"","jobId":23,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:46.815GMT","completionTime":"2023-10-07T21:49:47.606GMT","stageIds":[30,31],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1655,"dataRead":1788,"rowCount":8,"usageDescription":"","jobId":22,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:46.593GMT","completionTime":"2023-10-07T21:49:46.673GMT","stageIds":[29],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":21,"name":"","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:46.132GMT","completionTime":"2023-10-07T21:49:46.132GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":2019024,"dataRead":8075798,"rowCount":434754,"usageDescription":"","jobId":20,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:45.206GMT","completionTime":"2023-10-07T21:49:46.020GMT","stageIds":[27,28],"jobGroup":"9","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":8075798,"dataRead":10387094,"rowCount":434754,"usageDescription":"","jobId":19,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\")","submissionTime":"2023-10-07T21:49:44.394GMT","completionTime":"2023-10-07T21:49:45.160GMT","stageIds":[26],"jobGroup":"9","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4329,"rowCount":50,"usageDescription":"","jobId":18,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:44.123GMT","completionTime":"2023-10-07T21:49:44.181GMT","stageIds":[24,25,23],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4329,"dataRead":1503,"rowCount":54,"usageDescription":"","jobId":17,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\ndf_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_products\")\ndf_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_customers\")\ndf_invoices.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoices\")\ndf_invoice_details.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/bronze_invoice_details\"): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:49:43.183GMT","completionTime":"2023-10-07T21:49:44.082GMT","stageIds":[21,22],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0ee86b67-4072-49c8-8c07-110420ddc7e5"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 9, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"46e2de00-a6f7-4c97-857d-8de8940e5fa6"},{"cell_type":"code","source":["# create silver layer products table\n","df_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n","df_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n","df_silver_products.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:50:46.8312631Z","session_start_time":null,"execution_start_time":"2023-10-07T21:50:47.1264509Z","execution_finish_time":"2023-10-07T21:50:50.666233Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":9,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":1444,"rowCount":10,"usageDescription":"","jobId":54,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","submissionTime":"2023-10-07T21:50:49.747GMT","completionTime":"2023-10-07T21:50:49.819GMT","stageIds":[82],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1354,"rowCount":1,"usageDescription":"","jobId":53,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Filtering files for query","submissionTime":"2023-10-07T21:50:49.573GMT","completionTime":"2023-10-07T21:50:49.711GMT","stageIds":[81,80],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4334,"rowCount":50,"usageDescription":"","jobId":52,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 1","submissionTime":"2023-10-07T21:50:49.418GMT","completionTime":"2023-10-07T21:50:49.453GMT","stageIds":[78,79,77],"jobGroup":"11","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4334,"dataRead":3052,"rowCount":57,"usageDescription":"","jobId":51,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 1","submissionTime":"2023-10-07T21:50:48.841GMT","completionTime":"2023-10-07T21:50:49.402GMT","stageIds":[75,76],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3052,"dataRead":2458,"rowCount":14,"usageDescription":"","jobId":50,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Compute snapshot for version: 1","submissionTime":"2023-10-07T21:50:48.634GMT","completionTime":"2023-10-07T21:50:48.714GMT","stageIds":[74],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1731,"rowCount":3,"usageDescription":"","jobId":49,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","submissionTime":"2023-10-07T21:50:48.101GMT","completionTime":"2023-10-07T21:50:48.222GMT","stageIds":[72,73],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1874,"dataRead":338,"rowCount":20,"usageDescription":"","jobId":48,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","submissionTime":"2023-10-07T21:50:47.807GMT","completionTime":"2023-10-07T21:50:48.032GMT","stageIds":[70,71],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":338,"dataRead":1444,"rowCount":20,"usageDescription":"","jobId":47,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show()","submissionTime":"2023-10-07T21:50:47.691GMT","completionTime":"2023-10-07T21:50:47.768GMT","stageIds":[69],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1731,"rowCount":3,"usageDescription":"","jobId":46,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\ndf_silver_products = spark.read.format(\"delta\").load(\"Tables/bronze_products\")\n\ndf_silver_products.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/products\")\n\ndf_silver_products.show(): Filtering files for query","submissionTime":"2023-10-07T21:50:47.499GMT","completionTime":"2023-10-07T21:50:47.640GMT","stageIds":[67,68],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"37304ff1-766d-4c17-b8ba-1133bbad6695"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 11, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+---------+----------+\n|ProductId|  Product|  Category|\n+---------+---------+----------+\n|        1|   Apples|    Fruits|\n|        2|  Bananas|    Fruits|\n|        3|  Oranges|    Fruits|\n|        4|  Carrots|Vegetables|\n|        5|Cucumbers|Vegetables|\n|        6| Potatoes|Vegetables|\n|        7| Tomatoes|Vegetables|\n|        8|     Milk|     Dairy|\n|        9|   Butter|     Dairy|\n|       10|   Cheese|     Dairy|\n+---------+---------+----------+\n\n"]}],"execution_count":9,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"deae8c98-52fd-4f2a-aad9-2cef0bc118f9"},{"cell_type":"code","source":["# create silver layer customers table\n","from pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n","\n","df_silver_customers = (\n","\n","    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n","            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n","            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n","            .drop('FirstName', 'LastName')\n",")\n","\n","df_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n","\n","df_silver_customers.printSchema()\n","df_silver_customers.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:50:53.0588796Z","session_start_time":null,"execution_start_time":"2023-10-07T21:50:53.377697Z","execution_finish_time":"2023-10-07T21:50:56.9434592Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":9,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":2018191,"rowCount":4096,"usageDescription":"","jobId":63,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","submissionTime":"2023-10-07T21:50:56.592GMT","completionTime":"2023-10-07T21:50:56.704GMT","stageIds":[96],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1879,"rowCount":3,"usageDescription":"","jobId":62,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Filtering files for query","submissionTime":"2023-10-07T21:50:56.287GMT","completionTime":"2023-10-07T21:50:56.538GMT","stageIds":[94,95],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4368,"rowCount":50,"usageDescription":"","jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:50:56.044GMT","completionTime":"2023-10-07T21:50:56.075GMT","stageIds":[93,91,92],"jobGroup":"12","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4368,"dataRead":1640,"rowCount":54,"usageDescription":"","jobId":60,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:50:55.575GMT","completionTime":"2023-10-07T21:50:56.030GMT","stageIds":[89,90],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1640,"dataRead":1759,"rowCount":8,"usageDescription":"","jobId":59,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Compute snapshot for version: 0","submissionTime":"2023-10-07T21:50:55.388GMT","completionTime":"2023-10-07T21:50:55.448GMT","stageIds":[88],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":58,"name":"","description":"Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","submissionTime":"2023-10-07T21:50:54.980GMT","completionTime":"2023-10-07T21:50:54.980GMT","stageIds":[],"jobGroup":"12","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":4211629,"dataRead":6713806,"rowCount":434754,"usageDescription":"","jobId":57,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","submissionTime":"2023-10-07T21:50:54.170GMT","completionTime":"2023-10-07T21:50:54.848GMT","stageIds":[86,87],"jobGroup":"12","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":6713806,"dataRead":2018191,"rowCount":434754,"usageDescription":"","jobId":56,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show()","submissionTime":"2023-10-07T21:50:53.801GMT","completionTime":"2023-10-07T21:50:54.126GMT","stageIds":[85],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1879,"rowCount":3,"usageDescription":"","jobId":55,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 12:\nfrom pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n\ndf_silver_customers = (\n\n    spark.read.format(\"delta\").load(\"Tables/bronze_customers\")\n            .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n            .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n            .drop('FirstName', 'LastName')\n)\n\ndf_silver_customers.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/customers\")\n\ndf_silver_customers.printSchema()\ndf_silver_customers.show(): Filtering files for query","submissionTime":"2023-10-07T21:50:53.627GMT","completionTime":"2023-10-07T21:50:53.740GMT","stageIds":[84,83],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2c6732f8-f928-4ea1-bf35-03125fc2ce23"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 12, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- CustomerId: long (nullable = true)\n |-- Country: string (nullable = true)\n |-- City: string (nullable = true)\n |-- DOB: date (nullable = true)\n |-- Customer: string (nullable = false)\n |-- Age: long (nullable = true)\n\n+----------+-------+-------+----------+-----------------+---+\n|CustomerId|Country|   City|       DOB|         Customer|Age|\n+----------+-------+-------+----------+-----------------+---+\n|        16|Belgium|Antwerp|1998-04-03|       Lacy Cross| 25|\n|        57|Belgium|Antwerp|1992-04-02|     Cheryl Morse| 31|\n|        77|Belgium|Antwerp|1970-09-17|      Dolly Boyer| 53|\n|       133|Belgium|Antwerp|1991-06-13| Alyssa Frederick| 32|\n|       137|Belgium|Antwerp|1992-08-17|   Maxine Walters| 31|\n|       141|Belgium|Antwerp|1951-06-06|    Burl McCarthy| 72|\n|       147|Belgium|Antwerp|1983-11-20|     Eloise Bruce| 39|\n|       184|Belgium|Antwerp|2004-10-23|   Gwen Whitehead| 18|\n|       211|Belgium|Antwerp|1952-03-01|    Prince Franco| 71|\n|       221|Belgium|Antwerp|1944-02-21|  Guadalupe Lewis| 79|\n|       232|Belgium|Antwerp|1956-08-13|      Willie Pate| 67|\n|       268|Belgium|Antwerp|1963-06-17|     Graham Klein| 60|\n|       275|Belgium|Antwerp|1973-06-14|Houston Alexander| 50|\n|       300|Belgium|Antwerp|1945-07-07|    Lindsay Pitts| 78|\n|       319|Belgium|Antwerp|2002-02-22|         Elsa Cox| 21|\n|       329|Belgium|Antwerp|1989-03-20|   Rosanna Hodges| 34|\n|       359|Belgium|Antwerp|1999-08-09|    Hunter Newton| 24|\n|       413|Belgium|Antwerp|1966-01-06|     Alana Graves| 57|\n|       426|Belgium|Antwerp|1960-07-19|     Jamie Gibson| 63|\n|       427|Belgium|Antwerp|1957-11-19| Garrett Mitchell| 65|\n+----------+-------+-------+----------+-----------------+---+\nonly showing top 20 rows\n\n"]}],"execution_count":10,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"87a11595-0a6c-4b16-aa8d-8883217810e0"},{"cell_type":"code","source":["# create silver layer sales table\n","from pyspark.sql.functions import col, desc, concat, lit, floor, datediff\n","from pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n","\n","df_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\n","df_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n","\n","df_silver_sales = (\n","    df_bronze_invoice_details\n","            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n","            .withColumnRenamed('SalesAmount', 'Sales')\n","            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n","                               (month(col('Date'))*100) + \n","                               (dayofmonth(col('Date')))   )\n","\n","            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n","            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n",")\n","\n","df_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n","\n","df_silver_sales.printSchema()\n","df_silver_sales.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:51:59.7402445Z","session_start_time":null,"execution_start_time":"2023-10-07T21:52:00.0418546Z","execution_finish_time":"2023-10-07T21:52:08.467066Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":12,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":5597229,"rowCount":4096,"usageDescription":"","jobId":77,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","submissionTime":"2023-10-07T21:52:06.710GMT","completionTime":"2023-10-07T21:52:06.883GMT","stageIds":[117],"jobGroup":"14","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":1475,"rowCount":0,"usageDescription":"","jobId":76,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","submissionTime":"2023-10-07T21:52:06.579GMT","completionTime":"2023-10-07T21:52:06.707GMT","stageIds":[116],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":74,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","submissionTime":"2023-10-07T21:52:05.647GMT","completionTime":"2023-10-07T21:52:05.819GMT","stageIds":[114,113],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":73,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","submissionTime":"2023-10-07T21:52:05.456GMT","completionTime":"2023-10-07T21:52:05.570GMT","stageIds":[111,112],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4375,"rowCount":50,"usageDescription":"","jobId":72,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 0","submissionTime":"2023-10-07T21:52:05.332GMT","completionTime":"2023-10-07T21:52:05.358GMT","stageIds":[108,109,110],"jobGroup":"14","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4375,"dataRead":1344,"rowCount":54,"usageDescription":"","jobId":71,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 0","submissionTime":"2023-10-07T21:52:04.905GMT","completionTime":"2023-10-07T21:52:05.318GMT","stageIds":[107,106],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1344,"dataRead":1736,"rowCount":8,"usageDescription":"","jobId":70,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Compute snapshot for version: 0","submissionTime":"2023-10-07T21:52:04.727GMT","completionTime":"2023-10-07T21:52:04.780GMT","stageIds":[105],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":69,"name":"","description":"Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","submissionTime":"2023-10-07T21:52:04.344GMT","completionTime":"2023-10-07T21:52:04.344GMT","stageIds":[],"jobGroup":"14","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5538821,"dataRead":10895638,"rowCount":2483206,"usageDescription":"","jobId":68,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","submissionTime":"2023-10-07T21:52:03.137GMT","completionTime":"2023-10-07T21:52:04.264GMT","stageIds":[103,104],"jobGroup":"14","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":10895638,"dataRead":5598704,"rowCount":2483206,"usageDescription":"","jobId":67,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...","submissionTime":"2023-10-07T21:52:02.266GMT","completionTime":"2023-10-07T21:52:03.098GMT","stageIds":[102],"jobGroup":"14","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1800,"rowCount":3,"usageDescription":"","jobId":65,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","submissionTime":"2023-10-07T21:52:00.997GMT","completionTime":"2023-10-07T21:52:01.325GMT","stageIds":[99,100],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":1792,"rowCount":3,"usageDescription":"","jobId":64,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\nfrom pyspark.sql.functions import col, desc, concat, lit, floor, datediff\nfrom pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n\ndf_bronze_invoices = spark.read.format(\"delta\").load(\"Tables/bronze_invoices\")\ndf_bronze_invoice_details = spark.read.format(\"delta\").load(\"Tables/bronze_invoice_details\")\n\ndf_silver_sales = (\n    df_bronze_invoice_details\n            .join(df_bronze_invoices, df_bronze_invoice_details['InvoiceId'] == df_bronze_invoices['InvoiceId'])\n            .withColumnRenamed('SalesAmount', 'Sales')\n            .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n                               (month(col('Date'))*100) + \n                               (dayofmonth(col('Date')))   )\n\n            .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n            .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n)\n\ndf_silver_sales.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/sales\")\n\ndf_sil...: Filtering files for query","submissionTime":"2023-10-07T21:52:00.493GMT","completionTime":"2023-10-07T21:52:00.801GMT","stageIds":[97,98],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2e3e9168-ca4b-4a4b-acb9-b4f307445bc8"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 14, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- Date: date (nullable = true)\n |-- DateKey: integer (nullable = true)\n |-- CustomerId: long (nullable = true)\n |-- ProductId: long (nullable = true)\n |-- Sales: float (nullable = true)\n |-- Quantity: long (nullable = true)\n\n+----------+--------+----------+---------+-----+--------+\n|      Date| DateKey|CustomerId|ProductId|Sales|Quantity|\n+----------+--------+----------+---------+-----+--------+\n|2020-02-24|20200224|       606|       10| 90.0|      24|\n|2020-03-06|20200306|       776|       10| 90.0|      24|\n|2020-03-06|20200306|       789|       10| 90.0|      24|\n|2020-03-07|20200307|       803|       10| 90.0|      24|\n|2020-03-18|20200318|      1079|       10| 90.0|      24|\n|2020-04-02|20200402|      1663|       10| 90.0|      24|\n|2020-04-11|20200411|      1992|       10| 90.0|      24|\n|2020-04-12|20200412|      2079|       10| 90.0|      24|\n|2020-04-14|20200414|      2244|       10| 90.0|      24|\n|2020-04-29|20200429|      3055|       10| 90.0|      24|\n|2020-05-03|20200503|      3389|       10| 90.0|      24|\n|2020-05-07|20200507|      3633|       10| 90.0|      24|\n|2020-05-09|20200509|      3773|       10| 90.0|      24|\n|2020-05-11|20200511|      3823|       10| 90.0|      24|\n|2020-05-11|20200511|      3828|       10| 90.0|      24|\n|2020-05-13|20200513|      4036|       10| 90.0|      24|\n|2020-05-19|20200519|      4407|       10| 90.0|      24|\n|2020-05-21|20200521|      4551|       10| 90.0|      24|\n|2020-05-23|20200523|      4638|       10| 90.0|      24|\n|2020-05-26|20200526|      4774|       10| 90.0|      24|\n+----------+--------+----------+---------+-----+--------+\nonly showing top 20 rows\n\n"]}],"execution_count":12,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f98476ca-520e-441b-94e9-734b9f4e8e29"},{"cell_type":"code","source":["# create silver layer calendar table \n","import pyspark.pandas as ps\n","from datetime import datetime, timedelta, date\n","import os\n","import pandas as pd\n","\n","from pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n","\n","first_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\n","last_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n","\n","start_date = date(first_sales_date.year, 1, 1)\n","end_date = date(last_sales_date.year, 12, 31)\n","\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","df_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n","\n","df_calendar_spark = (\n","     spark.createDataFrame(df_calendar_ps)\n","       .withColumnRenamed(\"0\", \"timestamp\")\n","       .withColumn(\"Date\", to_date(col('timestamp')))\n","       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n","                              (month(col('timestamp'))*100) + \n","                              (dayofmonth(col('timestamp')))   )\n","       .withColumn(\"Year\", year(col('timestamp'))  )\n","       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n","       .withColumn(\"Month\", date_format(col('timestamp'),'yyyy-MM')  )\n","       .withColumn(\"Day\", dayofmonth(col('timestamp'))  )\n","       .withColumn(\"MonthInYear\", date_format(col('timestamp'),'MMMM')  )\n","       .withColumn(\"MonthInYearSort\", month(col('timestamp'))  )\n","       .withColumn(\"DayOfWeek\", date_format(col('timestamp'),'EEEE')  )\n","       .withColumn(\"DayOfWeekSort\", dayofweek(col('timestamp')))\n","       .drop('timestamp')\n",")\n","\n","df_calendar_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").format(\"delta\").save(f\"Tables/calendar\")\n","df_calendar_spark.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ec9e654c-e794-474a-b468-6f3aa6ed77d0","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-07T21:52:12.529255Z","session_start_time":null,"execution_start_time":"2023-10-07T21:52:12.8617346Z","execution_finish_time":"2023-10-07T21:52:19.6629674Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":15,"UNKNOWN":0,"RUNNING":0},"jobs":[{"displayName":"showString at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":94,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:18.320GMT","completionTime":"2023-10-07T21:52:18.352GMT","stageIds":[143],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4416,"rowCount":50,"usageDescription":"","jobId":93,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Compute snapshot for version: 0","submissionTime":"2023-10-07T21:52:18.233GMT","completionTime":"2023-10-07T21:52:18.260GMT","stageIds":[140,141,142],"jobGroup":"15","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4416,"dataRead":1767,"rowCount":54,"usageDescription":"","jobId":92,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Compute snapshot for version: 0","submissionTime":"2023-10-07T21:52:17.617GMT","completionTime":"2023-10-07T21:52:18.210GMT","stageIds":[139,138],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1767,"dataRead":2315,"rowCount":8,"usageDescription":"","jobId":91,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Compute snapshot for version: 0","submissionTime":"2023-10-07T21:52:17.457GMT","completionTime":"2023-10-07T21:52:17.502GMT","stageIds":[137],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":90,"name":"","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:17.061GMT","completionTime":"2023-10-07T21:52:17.061GMT","stageIds":[],"jobGroup":"15","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":22946,"dataRead":43264,"rowCount":2922,"usageDescription":"","jobId":89,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:16.670GMT","completionTime":"2023-10-07T21:52:16.951GMT","stageIds":[135,136],"jobGroup":"15","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":43264,"dataRead":0,"rowCount":1461,"usageDescription":"","jobId":88,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:16.536GMT","completionTime":"2023-10-07T21:52:16.632GMT","stageIds":[134],"jobGroup":"15","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:9","dataWritten":0,"dataRead":173,"rowCount":3,"usageDescription":"","jobId":87,"name":"collect at /tmp/ipykernel_8266/2758995502.py:9","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:15.766GMT","completionTime":"2023-10-07T21:52:15.787GMT","stageIds":[132,133],"jobGroup":"15","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:9","dataWritten":173,"dataRead":5595219,"rowCount":1241606,"usageDescription":"","jobId":86,"name":"collect at /tmp/ipykernel_8266/2758995502.py:9","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:15.348GMT","completionTime":"2023-10-07T21:52:15.691GMT","stageIds":[131],"jobGroup":"15","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:9","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":84,"name":"collect at /tmp/ipykernel_8266/2758995502.py:9","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","submissionTime":"2023-10-07T21:52:14.631GMT","completionTime":"2023-10-07T21:52:14.783GMT","stageIds":[129,128],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:9","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":83,"name":"collect at /tmp/ipykernel_8266/2758995502.py:9","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","submissionTime":"2023-10-07T21:52:14.410GMT","completionTime":"2023-10-07T21:52:14.545GMT","stageIds":[126,127],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:8","dataWritten":0,"dataRead":173,"rowCount":3,"usageDescription":"","jobId":82,"name":"collect at /tmp/ipykernel_8266/2758995502.py:8","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:14.270GMT","completionTime":"2023-10-07T21:52:14.299GMT","stageIds":[125,124],"jobGroup":"15","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:8","dataWritten":173,"dataRead":5595219,"rowCount":1241606,"usageDescription":"","jobId":81,"name":"collect at /tmp/ipykernel_8266/2758995502.py:8","description":"Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...","submissionTime":"2023-10-07T21:52:13.882GMT","completionTime":"2023-10-07T21:52:14.250GMT","stageIds":[123],"jobGroup":"15","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:8","dataWritten":0,"dataRead":1307,"rowCount":1,"usageDescription":"","jobId":79,"name":"collect at /tmp/ipykernel_8266/2758995502.py:8","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","submissionTime":"2023-10-07T21:52:13.134GMT","completionTime":"2023-10-07T21:52:13.270GMT","stageIds":[121,120],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8266/2758995502.py:8","dataWritten":0,"dataRead":1322,"rowCount":1,"usageDescription":"","jobId":78,"name":"collect at /tmp/ipykernel_8266/2758995502.py:8","description":"Delta: Job group for statement 15:\nimport pyspark.pandas as ps\nfrom datetime import datetime, timedelta, date\nimport os\nimport pandas as pd\n\nfrom pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek\n\nfirst_sales_date = df_silver_sales.agg({\"Date\": \"min\"}).collect()[0][0]\nlast_sales_date = df_silver_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n\nstart_date = date(first_sales_date.year, 1, 1)\nend_date = date(last_sales_date.year, 12, 31)\n\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\ndf_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n\ndf_calendar_spark = (\n     spark.createDataFrame(df_calendar_ps)\n       .withColumnRenamed(\"0\", \"timestamp\")\n       .withColumn(\"Date\", to_date(col('timestamp')))\n       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n                              (month(col('timestamp'))*100) + \n                              (dayofmonth(col('timestamp')))   )\n       .withColumn(\"Year\", year(col('timestamp'))  )\n       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-...: Filtering files for query","submissionTime":"2023-10-07T21:52:12.923GMT","completionTime":"2023-10-07T21:52:13.054GMT","stageIds":[118,119],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"dffbc0ec-f293-4ae9-adca-8a15a08a1b8e"},"text/plain":"StatementMeta(, ec9e654c-e794-474a-b468-6f3aa6ed77d0, 15, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"]},{"output_type":"stream","name":"stdout","text":["+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n|      Date| DateKey|Year|Quarter|  Month|Day|MonthInYear|MonthInYearSort|DayOfWeek|DayOfWeekSort|\n+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\n|2020-01-01|20200101|2020|2020-01|2020-01|  1|    January|              1|Wednesday|            4|\n|2020-01-02|20200102|2020|2020-01|2020-01|  2|    January|              1| Thursday|            5|\n|2020-01-03|20200103|2020|2020-01|2020-01|  3|    January|              1|   Friday|            6|\n|2020-01-04|20200104|2020|2020-01|2020-01|  4|    January|              1| Saturday|            7|\n|2020-01-05|20200105|2020|2020-01|2020-01|  5|    January|              1|   Sunday|            1|\n|2020-01-06|20200106|2020|2020-01|2020-01|  6|    January|              1|   Monday|            2|\n|2020-01-07|20200107|2020|2020-01|2020-01|  7|    January|              1|  Tuesday|            3|\n|2020-01-08|20200108|2020|2020-01|2020-01|  8|    January|              1|Wednesday|            4|\n|2020-01-09|20200109|2020|2020-01|2020-01|  9|    January|              1| Thursday|            5|\n|2020-01-10|20200110|2020|2020-01|2020-01| 10|    January|              1|   Friday|            6|\n|2020-01-11|20200111|2020|2020-01|2020-01| 11|    January|              1| Saturday|            7|\n|2020-01-12|20200112|2020|2020-01|2020-01| 12|    January|              1|   Sunday|            1|\n|2020-01-13|20200113|2020|2020-01|2020-01| 13|    January|              1|   Monday|            2|\n|2020-01-14|20200114|2020|2020-01|2020-01| 14|    January|              1|  Tuesday|            3|\n|2020-01-15|20200115|2020|2020-01|2020-01| 15|    January|              1|Wednesday|            4|\n|2020-01-16|20200116|2020|2020-01|2020-01| 16|    January|              1| Thursday|            5|\n|2020-01-17|20200117|2020|2020-01|2020-01| 17|    January|              1|   Friday|            6|\n|2020-01-18|20200118|2020|2020-01|2020-01| 18|    January|              1| Saturday|            7|\n|2020-01-19|20200119|2020|2020-01|2020-01| 19|    January|              1|   Sunday|            1|\n|2020-01-20|20200120|2020|2020-01|2020-01| 20|    January|              1|   Monday|            2|\n+----------+--------+----+-------+-------+---+-----------+---------------+---------+-------------+\nonly showing top 20 rows\n\n"]}],"execution_count":13,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"58438ae5-1d64-489c-9d0c-44edece5b09b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"5485cfa7-3f51-4f06-aabf-49f660e834a6"}],"default_lakehouse":"5485cfa7-3f51-4f06-aabf-49f660e834a6","default_lakehouse_name":"SalesDataLakehouse","default_lakehouse_workspace_id":"648e92d0-a6e0-4bcf-a4b7-00e16b056cc4"}}},"nbformat":4,"nbformat_minor":5}